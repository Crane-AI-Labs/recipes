{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZMCaGDIjF9n"
   },
   "source": [
    "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwkguWxljbIP"
   },
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-iicpNRjIoA"
   },
   "source": [
    "**NEW** Unsloth now supports training the new **gpt-oss** model from OpenAI! You can start finetune gpt-oss for free with our **[Colab notebook](https://x.com/UnslothAI/status/1953896997867729075)**!\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Read our **[Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NH7uu5ojTK8"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FA4RZ5f3lACa"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + \"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\"\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "fded9bb80c6146718bbe85413d95dde3",
      "dcec41b5762f40f6a13209f3733b4514",
      "7598e0cecdbd4f059c72dba5bcf169e6",
      "59b5db7fdef24034b2dcddc53e74e5db",
      "07d113d6ae3047aaba25be42f863d31b",
      "0d1f6b4539cd40c78e78c205345e24dd",
      "a3c33d9e448c4dc7a2b05fa4fdfc3dd4",
      "bd65c13273fa446c8e3eb86b7c660ca0",
      "4998298c0dc24871823af3bb90c0af19",
      "235b3cb584f343ffaa596c10e8d39c7c",
      "cb215e9005db4179b30acc4880e1e832",
      "3296ab4a3a8e4c49911f7008631a4b1c",
      "1a6ab3bdeb7441c894f303e72c389d67",
      "d387f946304e410b9d760b8aae23eca1",
      "eaa09937c3244dc68eadea5abe4297b2",
      "012451debabe4164bc591c3f4f4c8406",
      "eb801a813baf4ecbb3d237f2a217da83",
      "bdecaaf45051435799dd77a832d95ff2",
      "87f55e34c643482d9ad00bbc2f0d07ff",
      "505297a74bad4242ae20ca7b9e29b445"
     ]
    },
    "id": "WtXp4kEZeMVH",
    "outputId": "558ad94f-efe4-4317-c656-6ac1ebf2644e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fded9bb80c6146718bbe85413d95dde3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvb8Vt44jhqU"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429,
     "referenced_widgets": [
      "0094b99c269f4323b01e9608feb02453",
      "fa6d3db24bd348f499051ece1d20b89f",
      "5c2bd22f322d4f1cb06fed59cb72086e",
      "41e47a3940e84a3593ec851a20b008db",
      "9f51b4711868497385df67e45efc009f",
      "98e2a867d5784f5fa2262d61e4ae4dbd",
      "2c790a0c765b4ae9b2a7ef2ac2a73d79",
      "aec0b34e79d84193940d646d920f1e4e",
      "6514f028acdd46d9b6f5ea7aacfbd6fc",
      "abfade4962494d73b38406ca30aee9ab",
      "c680c96230e04323a943890abd54394d",
      "3dc3eebe4ce7459ba477380aa25c6de7",
      "354e2b41e3b044d0b45671e59c9f03c3",
      "5dc545c764f04f5f9d44f38c53b9616f",
      "c05989a7a5ef478a9b5d4e4c6be48145",
      "788cb3192ce24addba157ca9f72ddfbc",
      "4c3e6cad95b24c2ba4dec14ce9085fad",
      "b8b80f9bf06e4435baf1e838f2e25417",
      "43ee0c17a4784c83b0f114c86dad2b85",
      "8aaf3fff3aa448f1aeacdf873b0ac787",
      "5f0a3eba872a4f5d93af58ffa2a56935",
      "3ee06e515ae64963a0e9780f0fb114e8",
      "9dda9104e6404eb691abccdb0b57ffc8",
      "8953e138bb74493d9cbc5ab42a5c872a",
      "83ce355a21a84f4db376f8a126276e25",
      "01d3d11229084fc5b4ef806fa2f70e91",
      "47e1ba9659784eea8f03a952eea6c8e8",
      "dccfbd71c3194f88b6e015d6ad890533",
      "fe7d9a47e0b84baba70ba4e56692328a",
      "fada8ac9face4e4a889261b42ee2649c",
      "e9ae2f17c271465baae0b0a5aa8eda30",
      "4d627d566d2c4324b0a6e9961ad5ff7e",
      "cc9700a2b6c244dea749909ff023af7a",
      "85c59bee8f034163a397529a8beff7f1",
      "5d0096c79c504413b10c7e47f5c398ba",
      "1024927eec40487b9e0c50235e3d64cb",
      "5c2099bef9d94f08898006b084c101ae",
      "61f1c0fe26c24c5ebaefda3cea73bc79",
      "5fbbe2db22964bb3a253092db1d1fef5",
      "d0294680477f4f0393179fef25f57eae",
      "88011cd20edd4d2c8287b37d4e80531a",
      "d81ebfe288824bc8b990f2b00a1c4e98",
      "16b19c49832c4cf6975548b5f398f548",
      "627a56800ac8475ca357685aa0670ee2",
      "6ac38f8b653941d2aa30f015232a9ab5",
      "41470a5a8c10471ba3ed4e54d4b06e85",
      "9ae63434faa142df80fd21cc86bae7ec",
      "a67b973e32a84eb68977d1ebe70278c1",
      "3349c296e7b049918c42d04b4972a4db",
      "a529b50cea614bb096806737c20475d8",
      "fa5d271ca71b4237a34b92299bba0ea8",
      "5cfecac50fdc478ca4637645f093fceb",
      "e5539d06bdc24488afccda5e3040e77a",
      "08c0de32b8e849459597da9857c8c38f",
      "e4268576272b4fdd9226a7533ad1b673",
      "3dc9d1a9551a46ce955bb87b7bab2cf9",
      "d759af75ff53491c8a50f1c924210c75",
      "3f7ee1cae10b430f9ba40fb6358ebf49",
      "dfef44b53905461d90ad8b05e8fd24c3",
      "c531566ec5e746058f267339a2c41e94",
      "547c4f0272064e11ba3dd7ef5b15748b",
      "5c9e8cbc2ca34aaf87a4ef63eefe0974",
      "8350637faf4c4c35991b689361e81707",
      "6e981bd6a96c4f0ba719369543820dbf",
      "c6c6e935774948ac9a8418e846ee5823",
      "a7c2021a92b040f6a2c84ed3f6d1f56f",
      "f7c547437cb3429a99c1eb78beefaf36",
      "e9ff115b93344e6290c2412dd53e2b5d",
      "e67ded80ebb545928549a2d7a07ffb84",
      "5f29da7cce7545edb16197d77edc1a3d",
      "9d22ad1b7505493ab7f9a3b568d02611",
      "3db054ad92a14781844d79adec2233b6",
      "e719894d7f05410ebb4a1365956f0141",
      "af948aaeda0c41f69bb18075b5b89072",
      "4fdcd0fa54df45a0993aa7ab5e0fd5d9",
      "b533b5ed0a634dbdb62a82639a0ab2f4",
      "35ae15e5154e4e63880327bde9a9cd0f",
      "8b16f28ebd7643fc825695d44fa6b70b",
      "985d7dea13004cf38c10afdfe3637a18",
      "ceb2a3a714f64fbc8b57bfc76e416703",
      "2d63f1500b0b451393127847720f0456",
      "67cfc7d17acc4953a0b8e3e16ae717b8",
      "a115b5cd0ab54d2c8426a66364771931",
      "3d0f7ea071ec40b396590d1eb630de1d",
      "821f2483dd5748ce83c5e8ecf951d994",
      "1846adb03ca949b989d3559a36ca84e2",
      "cecf85fdf38a4ce9b9edf7902cb921b2",
      "8f5df95664c8409cb07a8d2210ea0c8e"
     ]
    },
    "id": "hnDC9T2Wim9y",
    "outputId": "78ca1088-d085-4060-b20d-4bc004a1d1e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.8.9: Fast Gemma3 patching. Transformers: 4.55.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0094b99c269f4323b01e9608feb02453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc3eebe4ce7459ba477380aa25c6de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dda9104e6404eb691abccdb0b57ffc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c59bee8f034163a397529a8beff7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac38f8b653941d2aa30f015232a9ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc9d1a9551a46ce955bb87b7bab2cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c547437cb3429a99c1eb78beefaf36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b16f28ebd7643fc825695d44fa6b70b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel  # FastVisionModel for LLMs\n",
    "import torch\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",  # Llama-3.1 2x faster\n",
    "    \"unsloth/Mistral-Small-Instruct-2409\",  # Mistral 22b 2x faster!\n",
    "    \"unsloth/Phi-4\",  # Phi-4 2x faster!\n",
    "    \"unsloth/Phi-4-unsloth-bnb-4bit\",  # Phi-4 Unsloth Dynamic 4-bit Quant\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",  # Gemma 2x faster!\n",
    "    \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"  # Qwen 2.5 2x faster!\n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",  # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "]  # More models at https://docs.unsloth.ai/get-started/all-our-models\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"CraneAILabs/swahili-gemma-1b\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUyXS-SQleFM"
   },
   "source": [
    "We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBIIyQbJlfs1",
    "outputId": "77d4bd12-b3df-4bc1-8e85-a30e57ace882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlmCuGLRm8XK"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We'll create our small, high-quality Luganda sentiment dataset. Unsloth works directly with Hugging Face `Dataset` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205,
     "referenced_widgets": [
      "2ba59a5bcedc4a249caeb63d8ea160fa",
      "8c1b2ba0a8ce4bc6b92bea91f13287e3",
      "779bed3625e343b6ae395cafd3d4c68e",
      "6fb2475cb20c43aabfd94c723acc8a5e",
      "cae025e723e34fb48243f1619128e502",
      "ed52e065a7314a59b5821feca1ff6b8f",
      "1adc890aa5b34ec1911e101c343314fe",
      "936d3dff611f4be69d61695e4c28131b",
      "da661ce462bd48ceac1c7d23506c7b0e",
      "cc026265f391487b8b0ad67854dd6bb8",
      "de7850018acb4e10810d112065ca3bbd",
      "f9abbbcfafbc41a19c1cbbd4f346645f",
      "912fc46ccc1041f3b3adc4f82a8449af",
      "25f86df80611433a84721641278fe134",
      "17509e9b86ff4dbe881f369c97305de8",
      "12d0510c51ba4db8ad7f08035f0c8c3d",
      "e557c9df083a45268d6450542d25f894",
      "e556d5e1cca34b769007ac31f7aa2090",
      "43b3a9ed0c214880a60dbc62cea00dbc",
      "3687a441bc0e4683a3c68ff598bfbb7e",
      "17d66fafa95f449b9922502d62fcbe8b",
      "9d4a7b45c8234782809b341302f1ff9b"
     ]
    },
    "id": "eJUqZFYgnPLK",
    "outputId": "e74bfc10-4cfb-4de3-f87a-3bcbbe50c700"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba59a5bcedc4a249caeb63d8ea160fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9abbbcfafbc41a19c1cbbd4f346645f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/279 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded and split successfully:\n",
      "Training samples: 251\n",
      "Testing samples: 28\n",
      "\n",
      "Example from the training set:\n",
      "{'text': '<bos><start_of_turn>user\\nAnalyze the sentiment of this Swahili sentence. The sentence is: \"Kompyuta imewashwa.\". The label is:<end_of_turn>\\n<start_of_turn>model\\nneutral<end_of_turn>\\n'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the path to your uploaded JSON file\n",
    "file_path = \"sentiment-swahili-cleaned.json\"\n",
    "\n",
    "# Load the dataset directly from the JSON file\n",
    "full_dataset = load_dataset('json', data_files=file_path)\n",
    "\n",
    "# The dataset is loaded as a DatasetDict, we'll work with the 'train' split\n",
    "dataset = full_dataset['train']\n",
    "\n",
    "# --- Formatting function ---\n",
    "def format_instruction(example):\n",
    "    instruction = f\"Analyze the sentiment of this Swahili sentence. The sentence is: \\\"{example['sentence']}\\\". The label is:\"\n",
    "    response = example['label']\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "        {\"role\": \"model\", \"content\": response}\n",
    "    ]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    return {\"text\": formatted_prompt}  # return as dict with \"text\" key\n",
    "\n",
    "# --- Map before splitting ---\n",
    "dataset = dataset.map(format_instruction, remove_columns=dataset.column_names)\n",
    "\n",
    "\n",
    "# --- BEST PRACTICE: Split the dataset into training and testing sets ---\n",
    "# This helps you evaluate how well your model generalizes to unseen data.\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "\n",
    "# Rename the splits for clarity\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']\n",
    "\n",
    "print(\"Dataset loaded and split successfully:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\\n\")\n",
    "\n",
    "print(\"Example from the training set:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQO1fHpC-HG2"
   },
   "source": [
    "Let's see how row 100 looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gj4gykfv98WF",
    "outputId": "2557ac41-f222-487a-fd47-3a5b6487f1ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<bos><start_of_turn>user\\nAnalyze the sentiment of this Swahili sentence. The sentence is: \"Ujumbe wako umenitia moyo.\". The label is:<end_of_turn>\\n<start_of_turn>model\\npositive<end_of_turn>\\n'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVGEDCwLnv0Y"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`! The training process remains almost identical to the standard approach, as Unsloth integrates seamlessly with the `SFTTrainer` from the `trl` library. We just need to define our formatting function and training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ca1u2NvdnxES"
   },
   "outputs": [],
   "source": [
    "# --- Formatting func for Unsloth (wrap in list here) ---\n",
    "def formatting_func(row):\n",
    "    return [row[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "cec939a4a32740049afb09a76dc1bcff",
      "09e7645ce3c6416da9d13584afc16274",
      "7a3d23d772394a03b46e526ba8951b02",
      "b587371e8bda4fc4ab807677d1a000e1",
      "9b816886a9e24e8b8967ccb94573cb17",
      "937f646db63148359fe252666cf14307",
      "be32d53bf256413e89c50221db337e26",
      "e3001d32f7204abb9e88f824a7c47ad5",
      "3aee12f384f34c158b42a3516da321d8",
      "99b3415e97184f529085317ffc45f864",
      "00cb4c7d91f949d6bb1dc442c56fcb12",
      "3433662c539541e6893846e3ec44da88",
      "123278bb36fd44f5831cba70220d833e",
      "7a7cdb9518074ace8d93e38c7e37e166",
      "79e4dc83937d4b8299cdd0eef55e6eef",
      "33fe9d2fedc44a6f9f2f72562f8b89f7",
      "96a087024b2b4480a56ebefd63ded40d",
      "3147009f3b70443aadbc2967efef54bc",
      "750e87d9b837462f84ab0c93ae9ba8ba",
      "d3cbaa33e7ba414f822dca9c6253cdb1",
      "a3cbcbcd4c6140c096785c092afae67b",
      "23dbc0fce9d2413da865c721cbeccc63"
     ]
    },
    "id": "DfpZcKVSomg_",
    "outputId": "f71be225-8e6c-4841-e661-26c9bc378ed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Switching to float32 training since model cannot work with float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec939a4a32740049afb09a76dc1bcff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/251 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3433662c539541e6893846e3ec44da88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/28 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# --- Train ---\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=512,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=False,\n",
    "        bf16=False,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UiHrm9Uo7Wm",
    "outputId": "dafca5cc-7396-48c2-c6eb-18c77a37c03f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla T4. Max memory = 14.741 GB.\n",
      "1.494 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "FZ4Il7lYpAVS",
    "outputId": "503f5845-a733-4180-911d-2e7e42f81778"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 251 | Num Epochs = 3 | Total steps = 96\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 13,045,760 of 1,012,931,712 (1.29% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 03:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.819300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.623300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.521000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.474900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.424200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.382200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.366900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3G1DXBNopF5P",
    "outputId": "9abd5c59-635f-455d-ea80-4a44d721ce84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248.445 seconds used for training.\n",
      "4.14 minutes used for training.\n",
      "Peak reserved memory = 1.494 GB.\n",
      "Peak reserved memory for training = 0.0 GB.\n",
      "Peak reserved memory % of max memory = 10.135 %.\n",
      "Peak reserved memory for training % of max memory = 0.0 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxyalK_1puCx"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can change the instruction and input - leave the output blank! Unsloth models are fully compatible with Hugging Face pipelines and `.generate()` methods.\n",
    "\n",
    "\n",
    "\n",
    "We use `temperature = 0.3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X8qmyxFQqO_t",
    "outputId": "44f0822b-ce1a-4ed3-f6c6-00449e9c29c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'Siungi mkono timu hii.'\n",
      "Predicted Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "# Create an inference prompt\n",
    "test_sentence = \"Siungi mkono timu hii.\" # \"I do not support this team.\"\n",
    "instruction = f\"Analyze the sentiment of this Swahili sentence. The sentence is: \\\"{test_sentence}\\\". The label is:\"\n",
    "messages = [{\"role\": \"user\", \"content\": instruction}, {\"role\": \"model\", \"content\": \"\"}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=5)\n",
    "generated_tokens = outputs[:, inputs.input_ids.shape[1]:]\n",
    "final_response = tokenizer.decode(generated_tokens[0], skip_special_tokens=True).strip()\n",
    "\n",
    "print(f\"Sentence: '{test_sentence}'\")\n",
    "print(f\"Predicted Sentiment: {final_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVmSa52Hq5Yl"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sz-rH5wFqpYd"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYClLAqyrbml"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecFJejmArD7s"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"model\")\n",
    "    tokenizer.save_pretrained(\"model\")\n",
    "if False:\n",
    "    model.push_to_hub(\"hf/model\", token = \"\")\n",
    "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9X-qN5ixrnoY"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list in our [Docs](https://docs.unsloth.ai/basics/saving-and-using-models/saving-to-gguf)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-Iv7l40rtvk"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVoPiZcrrz0r"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "Some other links:\n",
    "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
    "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
    "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
    "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
    "\n",
    "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
