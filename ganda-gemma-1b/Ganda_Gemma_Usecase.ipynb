{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlock the Power of Luganda with Ganda Gemma 1B!\n",
    "\n",
    "Dive into the world of Luganda artificial intelligence with this comprehensive guide to using `CraneAILabs/ganda-gemma-1b`. As a fine-tuned version of Google's Gemma 3 1B model, Ganda Gemma is specially designed for English-to-Luganda translation and conversational AI. This notebook will walk you through everything you need to know, from the model's capabilities to hands-on-code examples.\n",
    "\n",
    "### **Model at a Glance**\n",
    "\n",
    "*   **Model Name:** `CraneAILabs/ganda-gemma-1b`\n",
    "*   **Base Model:** `google/gemma-3-1b-it`\n",
    "*   **Parameters:** 1 Billion\n",
    "*   **Input Languages:** English and Luganda\n",
    "*   **Output Language:** Luganda\n",
    "*   **Primary Focus:** English-to-Luganda translation and Luganda conversational AI\n",
    "\n",
    "### **Capabilities: What Can Ganda Gemma 1B Do?**\n",
    "\n",
    "This powerful model is equipped with a range of capabilities to serve various linguistic needs:\n",
    "\n",
    "*   **English-to-Luganda Translation:** Seamlessly translate text from English to Luganda.\n",
    "*   **Conversational AI:** Engage in natural, human-like conversations in Luganda.\n",
    "*   **Text Summarization:** Condense longer Luganda texts into concise summaries.\n",
    "*   **Creative and Informational Writing:** Generate a variety of written content in Luganda.\n",
    "*   **Question Answering:** Provide answers to general knowledge questions in Luganda.\n",
    "\n",
    "### **Performance that Speaks for Itself**\n",
    "\n",
    "Ganda Gemma 1B has been rigorously evaluated, demonstrating impressive performance in translation quality. Here's a look at how it stacks up against other models based on BLEU and chrF++ scores, evaluated on 1,012 translation samples:\n",
    "\n",
    "| Model | BLEU Score | chrF++ |\n",
    "| :--- | :--- | :--- |\n",
    "| **Ganda Gemma 1B** | **6.98** | **40.63** |\n",
    "| ChatGPT-4o-latest | 6.46 | 39.98 |\n",
    "| Llama-4-Maverick | 4.29 | 33.52 |\n",
    "| Llama-4-Scout | 3.28 | 27.33 |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Let's Get Coding!\n",
    "\n",
    "### **Setup: Installing the Essentials**\n",
    "\n",
    "First, let's make sure you have the necessary libraries installed. We'll need `transformers` for interacting with the model and `torch` for the underlying computations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Use Case 1: English-to-Luganda Translation**\n",
    "\n",
    "Translating from English to Luganda is a core feature of Ganda Gemma 1B. Hereâ€™s how you can do it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"CraneAILabs/ganda-gemma-1b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chat-formatted prompt\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Translate to Luganda: Good morning, how did you sleep?\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate new tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.3,\n",
    "        top_p=0.95,\n",
    "        top_k=64,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# The generated output will now contain both the formatted prompt and the answer.\n",
    "# We still need to decode and then extract just the model's response.\n",
    "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract only the model's part of the response\n",
    "# Gemma's response starts after '<start_of_turn>model\\n'\n",
    "model_response = full_response.split('<start_of_turn>model\\n')[-1]\n",
    "\n",
    "print(f\"Model's Response: {model_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"CraneAILabs/ganda-gemma-1b\"\n",
    "\n",
    "# Load the tokenizer separately to build the prompt\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# --- Most Robust Pipeline for CPU ---\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=\"auto\",\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "# 1. Apply the chat template just like before\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Translate to Luganda: Welcome to our school\"}\n",
    "]\n",
    "prompt_from_template = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# 2. Pass the formatted prompt to the generator\n",
    "result = generator(\n",
    "    prompt_from_template,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.3,\n",
    "    do_sample=True,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "print(\"\\n--- Pipeline Translation ---\")\n",
    "print(f\"Generated Text: '{result[0]['generated_text']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Use Case 2: Luganda Conversational AI**\n",
    "\n",
    "Engage directly with the model in Luganda for a conversational experience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Direct Luganda Conversation ---\n",
    "prompt = \"Mwasuze mutya! Nnyinza ntya okukuyamba leero?\" # (Good morning! How can I help you today?)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=100, temperature=0.3)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"User Prompt: {prompt}\")\n",
    "print(f\"Ganda Gemma's Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"CraneAILabs/ganda-gemma-1b\"\n",
    "\n",
    "# Load the tokenizer separately. This is crucial for using the chat template.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create the text-generation pipeline with CPU optimizations\n",
    "# torch_dtype=\"auto\" helps torch select the right data type for your hardware (e.g., float32 for CPU)\n",
    "chat_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=\"auto\",\n",
    "    device=-1  # Use -1 to explicitly set to CPU\n",
    ")\n",
    "\n",
    "print(\"Chat pipeline is ready. You can now start the conversation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Ganda Gemma Interactive Chat ---\")\n",
    "print(\"Model is ready. Type your message or a translation request.\")\n",
    "print(\"Type 'quit' or 'exit' to end the chat.\\n\")\n",
    "\n",
    "while True:\n",
    "    # 1. Get input from the user\n",
    "    user_input = input(\"You: \")\n",
    "\n",
    "    # 2. Check for an exit command\n",
    "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
    "        print(\"Goodbye! The chat session has ended.\")\n",
    "        break\n",
    "\n",
    "    # 3. Format the input using the model's official chat template\n",
    "    # This is the most reliable way to get good responses from Gemma models.\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    prompt = chat_pipeline.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # 4. Generate a response using the pipeline\n",
    "    # We use 'max_new_tokens' and 'return_full_text=False' to get a clean response.\n",
    "    response = chat_pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=150,\n",
    "        do_sample=True,\n",
    "        temperature=0.7, # A slightly higher temperature can make chat more interesting\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        return_full_text=False\n",
    "    )\n",
    "\n",
    "    # 5. Print the model's generated response\n",
    "    print(f\"Ganda Gemma: {response[0]['generated_text'].strip()}\")\n",
    "    print(\"-\" * 20) # Separator for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### **Use Case 4: Text Summarization in Luganda**\n",
    "\n",
    "Beyond translation, Ganda Gemma can understand and process Luganda text to extract the most important information. This is incredibly useful for condensing long articles, reports, or paragraphs into a short, easy-to-read summary.\n",
    "\n",
    "We'll use prompt engineering to instruct the model to act as a summarizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- The Text to Summarize ---\n",
    "long_luganda_text = \"\"\"\n",
    "Ebyenjigiriza bya muwendo nnyo mu bulamu bwa buli muntu ne mu nkulaakulana y'eggwanga. Biyamba abantu okufuna amagezi, obukugu, n'empisa ennungi ebiyamba okweyimirizaawo. Okuva ku baana abato okutuuka ku bantu abakulu, okusoma kubayamba okumanya ebigenda mu maaso mu nsi, okutegeera eddembe lyabwe, n'okuba ab'omugaso mu kitundu. Eggwanga eririna abantu abasomye bulungi lisobola okukulaakulana mangu mu by'enfuna, eby'obufuzi, n'embeera z'abantu.\n",
    "\"\"\"\n",
    "\n",
    "# --- The Instruction (Prompt) ---\n",
    "# We tell the model exactly what to do with the text.\n",
    "instruction = f\"Summarize the following Luganda text into one concise sentence: \\n\\n{long_luganda_text}\"\n",
    "\n",
    "# Format using the chat template for best results\n",
    "messages = [{\"role\": \"user\", \"content\": instruction}]\n",
    "prompt = chat_pipeline.tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Generate the summary\n",
    "# We use a low temperature for fact-based, non-creative output.\n",
    "summary_result = chat_pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.2, # Low temperature for factual summary\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "print(\"--- Luganda Text Summarization ---\")\n",
    "print(f\"Original Text:\\n{long_luganda_text.strip()}\")\n",
    "print(\"\\n\" + \"=\"*25 + \"\\n\")\n",
    "print(f\"Generated Summary:\\n{summary_result[0]['generated_text'].strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Use Case 5: Creative and Informational Writing**\n",
    "\n",
    "Need to write a short story, a poem, or an informational paragraph in Luganda? Ganda Gemma can be your creative partner. By providing a topic or a starting sentence, you can generate a variety of written content.\n",
    "\n",
    "For creative tasks, we'll increase the `temperature` parameter slightly to encourage the model to generate more diverse and interesting text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Creative Writing: A Short Story ---\n",
    "story_idea = \"Write a short story in Luganda about a clever rabbit (akamyu) and a friendly elephant (enjovu).\"\n",
    "\n",
    "messages_story = [{\"role\": \"user\", \"content\": story_idea}]\n",
    "prompt_story = chat_pipeline.tokenizer.apply_chat_template(\n",
    "    messages_story, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Use a higher temperature for more creative and unpredictable results\n",
    "creative_result = chat_pipeline(\n",
    "    prompt_story,\n",
    "    max_new_tokens=250, # Allow for a longer story\n",
    "    do_sample=True,\n",
    "    temperature=0.8, # Higher temperature for creativity\n",
    "    top_p=0.95,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "print(\"--- Creative Writing Example ---\")\n",
    "print(f\"Prompt: {story_idea}\")\n",
    "print(\"\\n\" + \"=\"*25 + \"\\n\")\n",
    "print(f\"Generated Story:\\n{creative_result[0]['generated_text'].strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Informational Writing: Factual Content ---\n",
    "info_idea = \"Write a short paragraph in Luganda explaining the importance of boiling drinking water.\"\n",
    "\n",
    "messages_info = [{\"role\": \"user\", \"content\": info_idea}]\n",
    "prompt_info = chat_pipeline.tokenizer.apply_chat_template(\n",
    "    messages_info, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Use a lower temperature for factual, informational text\n",
    "info_result = chat_pipeline(\n",
    "    prompt_info,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.3, # Lower temperature for factual content\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "print(\"\\n\\n--- Informational Writing Example ---\")\n",
    "print(f\"Prompt: {info_idea}\")\n",
    "print(\"\\n\" + \"=\"*25 + \"\\n\")\n",
    "print(f\"Generated Paragraph:\\n{info_result[0]['generated_text'].strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Use Case 6: General Knowledge Question Answering**\n",
    "\n",
    "You can ask Ganda Gemma general knowledge questions directly in Luganda. The model will tap into the vast information it learned during its pre-training to provide an answer.\n",
    "\n",
    "This is another form of zero-shot learning. For the best factual recall, it's important to keep the `temperature` low to prevent the model from making things up (hallucinating).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ask a Question in Luganda ---\n",
    "question = \"Kibuga ki ekikulu ekya Uganda?\" # \"What is the capital city of Uganda?\"\n",
    "\n",
    "# Format the question using the chat template.\n",
    "messages_qa = [{\"role\": \"user\", \"content\": question}]\n",
    "prompt_qa = chat_pipeline.tokenizer.apply_chat_template(\n",
    "    messages_qa, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Generate the answer with a low temperature for accuracy.\n",
    "answer_result = chat_pipeline(\n",
    "    prompt_qa,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.1, # Very low temperature for factual answers\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "print(\"--- Question Answering Example ---\")\n",
    "print(f\"Question: {question}\")\n",
    "print(\"\\n\" + \"=\"*25 + \"\\n\")\n",
    "print(f\"Generated Answer: {answer_result[0]['generated_text'].strip()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Another Example ---\n",
    "question_2 = \"Lwaki ebirime byetaaga amazzi okukula?\" # \"Why do crops need water to grow?\"\n",
    "\n",
    "messages_qa_2 = [{\"role\": \"user\", \"content\": question_2}]\n",
    "prompt_qa_2 = chat_pipeline.tokenizer.apply_chat_template(\n",
    "    messages_qa_2, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "answer_result_2 = chat_pipeline(\n",
    "    prompt_qa_2,\n",
    "    max_new_tokens=150,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "print(\"\\n\\n--- Another Question Answering Example ---\")\n",
    "print(f\"Question: {question_2}\")\n",
    "print(\"\\n\" + \"=\"*25 + \"\\n\")\n",
    "print(f\"Generated Answer: {answer_result_2[0]['generated_text'].strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optimal Generation Parameters**\n",
    "\n",
    "To get the best results from Ganda Gemma 1B, it's recommended to use the following generation parameters:\n",
    "\n",
    "*   **`temperature`: 0.3** - For focused and coherent responses.\n",
    "*   **`top_p`: 0.95** - Utilizes nucleus sampling.\n",
    "*   **`top_k`: 64** - Employs top-k sampling.\n",
    "*   **`max_length`: 128** - Sets a limit on the response length.\n",
    "*   **`repetition_penalty`: 1.1** - Helps in reducing word repetition.\n",
    "*   **`do_sample`: True** - Enables sampling for more dynamic responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Important Limitations to Consider**\n",
    "\n",
    "While Ganda Gemma 1B is a powerful tool, it's important to be aware of its limitations:\n",
    "\n",
    "*   **Luganda-Only Output:** The model is designed to respond exclusively in Luganda.\n",
    "*   **General Knowledge Base:** It has not been trained on specific factual datasets, so its knowledge is general.\n",
    "*   **No Coding or Math:** The model is not designed for programming or mathematical tasks.\n",
    "*   **Context Length:** For optimal performance, the context length is limited to 4,096 tokens.\n",
    "*   **Domain-Specific Fine-Tuning:** For specialized domains, further fine-tuning may be required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **License Information**\n",
    "\n",
    "The Ganda Gemma 1B model is released under the Gemma Terms of Use. Please review the terms before using the model.\n",
    "\n",
    "We hope this notebook serves as an excellent starting point for your journey with Luganda AI. Happy coding"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
