{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
    "</div>\n",
    "\n",
    "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NEW** Unsloth now supports training the new **gpt-oss** model from OpenAI! You can start finetune gpt-oss for free with our **[Colab notebook](https://x.com/UnslothAI/status/1953896997867729075)**!\n",
    "\n",
    "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
    "\n",
    "Read our **[Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Setup: Install Unsloth and Dependencies**\n",
    "\n",
    "Unsloth's installation is specific to your GPU's architecture. The following command is optimized for Google Colab GPUs (like T4, V100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth for Colab notebooks\n",
    "#!pip install \"unsloth[colab-new] @ git+https://github.com/unsloth/unsloth.git\"\n",
    "!pip install -q transformers datasets peft trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
    "    !pip install --no-deps unsloth vllm==0.8.5.post1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Colab Extra Install { display-mode: \"form\" }\n",
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    !pip install --no-deps unsloth vllm==0.8.5.post1\n",
    "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
    "    # Skip restarting message in Colab\n",
    "    import sys, re, requests; modules = list(sys.modules.keys())\n",
    "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "\n",
    "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
    "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
    "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
    "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
    "    !pip install -r vllm_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Prepare the Dataset**\n",
    "\n",
    "We'll create our small, high-quality Luganda sentiment dataset. Unsloth works directly with Hugging Face `Dataset` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the path to your uploaded JSON file\n",
    "file_path = \"sentiment-luganda.json\"\n",
    "\n",
    "# Load the dataset directly from the JSON file\n",
    "full_dataset = load_dataset('json', data_files=file_path)\n",
    "\n",
    "# The dataset is loaded as a DatasetDict, we'll work with the 'train' split\n",
    "dataset = full_dataset['train']\n",
    "\n",
    "# --- Formatting function ---\n",
    "def format_instruction(example):\n",
    "    instruction = f\"Analyze the sentiment of this Luganda sentence. The sentence is: \\\"{example['sentence']}\\\". The label is:\"\n",
    "    response = example['label']\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "        {\"role\": \"model\", \"content\": response}\n",
    "    ]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    return {\"text\": formatted_prompt}  # return as dict with \"text\" key\n",
    "\n",
    "# --- Map before splitting ---\n",
    "dataset = dataset.map(format_instruction, remove_columns=dataset.column_names)\n",
    "\n",
    "\n",
    "# --- BEST PRACTICE: Split the dataset into training and testing sets ---\n",
    "# This helps you evaluate how well your model generalizes to unseen data.\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "\n",
    "# Rename the splits for clarity\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']\n",
    "\n",
    "print(\"Dataset loaded and split successfully:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\\n\")\n",
    "\n",
    "print(\"Example from the training set:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you do not have a training dataset, you can use this one**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Our custom Luganda dataset\n",
    "data = {\n",
    "    'omutwe': [ # 'omutwe' means 'sentence' or 'text'\n",
    "        \"Njagala nnyo filimu eno.\", \"Emmere ebadde ewooma.\", \"Olunaku luno lwali lwa ssanyu.\",\n",
    "        \"Sitwagala n'akatono filimu eno.\", \"Emmere ebadde mbi nnyo.\", \"Nali munakuwavu nnyo.\",\n",
    "        \"Leero olunaku lwa Lwakusatu.\", \"Mmotoka eno ya myaka kkumi.\", \"Enkuba etonnya.\",\n",
    "        \"Yazaalidwa mu mwezi gwa Mukutulansanja.\", \"Anonya mulimu.\", \"Olunaku lwabadde lwa bulijjo.\",\n",
    "        \"Ekitabo kino kinyuma nnyo!\", \"Babadde bantu balungi nnyo.\"\n",
    "    ],\n",
    "    'omuwendo': [ # 'omuwendo' means 'label' or 'value'\n",
    "        \"Kirungi\", \"Kirungi\", \"Kirungi\",\n",
    "        \"Kibi\", \"Kibi\", \"Kibi\",\n",
    "        \"Kya bulijjo\", \"Kya bulijjo\", \"Kya bulijjo\",\n",
    "        \"Kya bulijjo\", \"Kya bulijjo\", \"Kya bulijjo\",\n",
    "        \"Kirungi\", \"Kirungi\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to a Hugging Face Dataset object\n",
    "luganda_sentiment_dataset = Dataset.from_dict(data)\n",
    "\n",
    "print(\"Dataset created successfully:\")\n",
    "print(luganda_sentiment_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *\n",
    "\n",
    "### **3. Load the Model Using Unsloth**\n",
    "\n",
    "This is where the magic of Unsloth begins. We'll use `FastLanguageModel` to load our model. Notice how much simpler it is—Unsloth handles the quantization, data types, and device mapping automatically under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 1024 # Can increase for longer reasoning traces\n",
    "lora_rank = 32 # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"CraneAILabs/ganda-gemma-1b\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.7, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
    "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Fine-Tuning with the TRL Trainer**\n",
    "\n",
    "The training process remains almost identical to the standard approach, as Unsloth integrates seamlessly with the `SFTTrainer` from the `trl` library. We just need to define our formatting function and training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Formatting func for Unsloth (wrap in list here) ---\n",
    "def formatting_func(row):\n",
    "    return [row[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train ---\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=512,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=False,\n",
    "        bf16=False,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Start training! Keep an eye on the speed (tokens/sec).\n",
    "print(\"Starting the Unsloth fine-tuning process...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *\n",
    "\n",
    "### **5. Inference with the Fine-Tuned Unsloth Model**\n",
    "\n",
    "Now, let's use our specialized model for prediction. Unsloth models are fully compatible with Hugging Face pipelines and `.generate()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an inference prompt\n",
    "test_sentence = \"Siwagira ttiimu eno.\" # \"I do not support this team.\"\n",
    "instruction = f\"Analyze the sentiment of this Luganda sentence. The sentence is: \\\"{test_sentence}\\\". The label is:\"\n",
    "messages = [{\"role\": \"user\", \"content\": instruction}, {\"role\": \"model\", \"content\": \"\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=5)\n",
    "generated_tokens = outputs[:, inputs.input_ids.shape[1]:]\n",
    "final_response = tokenizer.decode(generated_tokens[0], skip_special_tokens=True).strip()\n",
    "\n",
    "print(f\"Sentence: '{test_sentence}'\")\n",
    "print(f\"Predicted Sentiment: {final_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Another Test ---\n",
    "test_sentence_pos = \"Oluyimba luno lunyuma nnyo.\" # \"This song is very nice.\"\n",
    "instruction = f\"Analyze the sentiment of this Luganda sentence. The sentence is: \\\"{test_sentence_pos}\\\". The label is:\"\n",
    "messages = [{\"role\": \"user\", \"content\": instruction}, {\"role\": \"model\", \"content\": \"\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=5)\n",
    "generated_tokens = outputs[:, inputs.input_ids.shape[1]:]\n",
    "final_response = tokenizer.decode(generated_tokens[0], skip_special_tokens=True).strip()\n",
    "\n",
    "print(f\"Sentence: '{test_sentence_pos}'\")\n",
    "print(f\"Predicted Sentiment: {final_response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Saving the Model**\n",
    "\n",
    "You can save the trained LoRA adapters in the standard Hugging Face format, making them portable and easy to use elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained LoRA adapters\n",
    "model.save_pretrained(\"luganda_sentiment_lora\")\n",
    "\n",
    "print(\"Model adapters saved to 'luganda_sentiment_lora'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You can also push them to the Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"yourusername/luganda-sentiment-gemma-unsloth\", token=\"hf_...\")\n",
    "\n",
    "tokenizer.push_to_hub(\"yourusername/luganda-sentiment-gemma-unsloth\", token=\"hf_..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "Some other links:\n",
    "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
    "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
    "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
    "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
    "\n",
    "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
