{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Luganda YouTube Comments Sentiment Analysis with Ganda Gemma\n",
        "\n",
        "**Project Overview:**\n",
        "This notebook demonstrates sentiment analysis of Luganda (Ugandan language) YouTube comments using the CraneAILabs Ganda Gemma model. We classify comments as \"Kirungi\" (good/positive) or \"Kibi\" (bad/negative) to help Ugandan content creators understand their audience sentiment.\n",
        "\n",
        "**Key Features:**\n",
        "- YouTube Data API integration for comment extraction\n",
        "- Ganda Gemma model for Luganda sentiment analysis\n",
        "- Authentic Ugandan sentiment labels (Kirungi/Kibi)\n",
        "- Real-world application for content creators\n",
        "\n",
        "**Date:** August 2025  \n",
        "**Model:** CraneAILabs/ganda-gemma-1b"
      ],
      "metadata": {
        "id": "CYc5miTXLvi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. SETUP AND INSTALLATIONS"
      ],
      "metadata": {
        "id": "XWj5htWJL1u_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "#\"\"\"\n",
        "!pip install transformers torch google-api-python-client pandas matplotlib seaborn plotly\n",
        "!pip install huggingface_hub python-dotenv wordcloud\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import glob\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from datetime import datetime, timedelta\n",
        "from collections import Counter, defaultdict\n",
        "import string\n",
        "\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Transformers and Google API\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ All packages installed and imported successfully!\")\n",
        "\n",
        "#\"\"\""
      ],
      "metadata": {
        "id": "HJVWnwIsL0Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. AUTHENTICATION AND API SETUP"
      ],
      "metadata": {
        "id": "fJ7de9NbMzCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Hugging Face Authentication"
      ],
      "metadata": {
        "id": "0fCuGENgOMPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#First, we authenticate with Hugging Face to access the private Ganda Gemma model.\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "HF_TOKEN = userdata.get('Ganda_Gemma_Token')\n",
        "login(token=HF_TOKEN)\n",
        "print(\"Hugging Face authentication successful!\")"
      ],
      "metadata": {
        "id": "IEJrrZZ6MtGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 YouTube Data API Setup"
      ],
      "metadata": {
        "id": "ERnTc8QpOGLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up YouTube API for comment extraction.\n",
        "YOUTUBE_API_KEY = userdata.get('YouTube_API_Key')\n",
        "youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)\n",
        "print(\"YouTube API authentication successful!\")"
      ],
      "metadata": {
        "id": "MkXegmqtNua7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. DATA COLLECTION"
      ],
      "metadata": {
        "id": "_I6rKnk9NMDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Clean Uganda YouTube Comment Extractor\n",
        "Rewritten with proper indentation and structure\n",
        "\"\"\"\n",
        "class CleanUgandaCommentExtractor:\n",
        "    def __init__(self, api_key):\n",
        "        \"\"\"Initialize with YouTube API key.\"\"\"\n",
        "        self.api_key = api_key\n",
        "        self.youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "        # Target channels (2 per category)\n",
        "        self.target_channels = {\n",
        "            'Music & Entertainment': [\n",
        "                {'name': 'UGXTRA Music', 'search_term': 'UGXTRA Music'},\n",
        "                {'name': 'Elijah Kitaka', 'search_term': 'Elijah Kitaka'}\n",
        "            ],\n",
        "            'Comedy & Lifestyle': [\n",
        "                {'name': 'UGXTRA Comedy', 'search_term': 'UGXTRA Comedy'},\n",
        "                {'name': 'Comedy Store', 'search_term': 'Comedy Store Uganda'}\n",
        "            ],\n",
        "            'News & Current Affairs': [\n",
        "                {'name': 'Kasuku Live', 'search_term': 'Kasuku Live'},\n",
        "                {'name': 'NTV Akawungeezi', 'search_term': 'NTV Akawungeezi'}\n",
        "            ],\n",
        "            'Sports & Events': [\n",
        "                {'name': 'Uganda Fan TV', 'search_term': 'Uganda Fan TV'},\n",
        "                {'name': 'KWEZI MEDIA GROUP', 'search_term': 'KWEZI MEDIA GROUP'}\n",
        "            ],\n",
        "            'Politics & Social Issues': [\n",
        "                {'name': 'Ug Wolokoso Extra', 'search_term': 'Ug Wolokoso Extra'},\n",
        "                {'name': 'BBS Terefayina', 'search_term': 'BBS Terefayina'}\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def find_channel_id(self, search_term):\n",
        "        \"\"\"Find channel ID using multiple search strategies - Uganda only.\"\"\"\n",
        "        try:\n",
        "            # Strategy 1: Direct search with Uganda region\n",
        "            request = self.youtube.search().list(\n",
        "                part=\"snippet\",\n",
        "                q=search_term,\n",
        "                type=\"channel\",\n",
        "                maxResults=10,  # Increased from 5\n",
        "                regionCode=\"UG\"\n",
        "            )\n",
        "            response = request.execute()\n",
        "\n",
        "            if response['items']:\n",
        "                return response['items'][0]['snippet']['channelId']\n",
        "\n",
        "            # Strategy 2: Try variations of the search term (still Uganda only)\n",
        "            variations = [\n",
        "                search_term.replace(\" Official\", \"\"),\n",
        "                search_term.replace(\" \", \"\"),\n",
        "                f\"{search_term} channel\",\n",
        "                f\"{search_term} TV\",\n",
        "                f\"{search_term} Uganda\"\n",
        "            ]\n",
        "\n",
        "            for variation in variations:\n",
        "                print(f\"   üîÑ Trying Uganda variation: {variation}\")\n",
        "                request = self.youtube.search().list(\n",
        "                    part=\"snippet\",\n",
        "                    q=variation,\n",
        "                    type=\"channel\",\n",
        "                    maxResults=10,\n",
        "                    regionCode=\"UG\"  # Keep Uganda region code\n",
        "                )\n",
        "                response = request.execute()\n",
        "\n",
        "                if response['items']:\n",
        "                    return response['items'][0]['snippet']['channelId']\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error finding channel '{search_term}': {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_trending_video(self, channel_id, channel_name):\n",
        "        \"\"\"Get the most trending video from a channel with flexible criteria.\"\"\"\n",
        "        try:\n",
        "            # Try different time ranges if needed\n",
        "            time_ranges = [\n",
        "                (180, \"6 months\"),  # Original\n",
        "                (365, \"1 year\"),    # Expand to 1 year\n",
        "                (730, \"2 years\")    # Expand to 2 years if needed\n",
        "            ]\n",
        "\n",
        "            for days, period_name in time_ranges:\n",
        "                print(f\"   üîç Searching {period_name} of videos...\")\n",
        "\n",
        "                time_ago = (datetime.now() - timedelta(days=days)).isoformat() + 'Z'\n",
        "\n",
        "                request = self.youtube.search().list(\n",
        "                    part=\"snippet\",\n",
        "                    channelId=channel_id,\n",
        "                    type=\"video\",\n",
        "                    order=\"relevance\",\n",
        "                    maxResults=20,  # Increased from 10\n",
        "                    publishedAfter=time_ago\n",
        "                )\n",
        "                response = request.execute()\n",
        "\n",
        "                if not response['items']:\n",
        "                    continue\n",
        "\n",
        "                # Get video statistics with flexible comment threshold\n",
        "                video_stats = []\n",
        "                for item in response['items']:\n",
        "                    video_id = item['id']['videoId']\n",
        "                    title = item['snippet']['title']\n",
        "\n",
        "                    # Get detailed stats\n",
        "                    stats_request = self.youtube.videos().list(\n",
        "                        part=\"statistics\",\n",
        "                        id=video_id\n",
        "                    )\n",
        "                    stats_response = stats_request.execute()\n",
        "\n",
        "                    if stats_response['items']:\n",
        "                        stats = stats_response['items'][0]['statistics']\n",
        "                        comment_count = int(stats.get('commentCount', 0))\n",
        "                        view_count = int(stats.get('viewCount', 0))\n",
        "\n",
        "                        # Flexible comment threshold: try 50+, then 20+, then 10+\n",
        "                        min_comments = 50 if days <= 180 else (20 if days <= 365 else 10)\n",
        "\n",
        "                        if comment_count >= min_comments:\n",
        "                            engagement_score = comment_count + (view_count * 0.001)\n",
        "                            video_stats.append({\n",
        "                                'video_id': video_id,\n",
        "                                'title': title,\n",
        "                                'comment_count': comment_count,\n",
        "                                'view_count': view_count,\n",
        "                                'engagement_score': engagement_score,\n",
        "                                'period': period_name\n",
        "                            })\n",
        "\n",
        "                if video_stats:\n",
        "                    # Found videos with comments - return the best one\n",
        "                    trending_video = max(video_stats, key=lambda x: x['engagement_score'])\n",
        "                    print(f\"   üìπ Found: '{trending_video['title'][:50]}...' ({trending_video['comment_count']} comments, {trending_video['period']})\")\n",
        "                    return trending_video\n",
        "\n",
        "                print(f\"   ‚ö†Ô∏è  No videos with {50 if days <= 180 else (20 if days <= 365 else 10)}+ comments in {period_name}\")\n",
        "\n",
        "            # If still no luck, get ANY video with comments\n",
        "            print(f\"   üîÑ Trying any video with comments...\")\n",
        "            request = self.youtube.search().list(\n",
        "                part=\"snippet\",\n",
        "                channelId=channel_id,\n",
        "                type=\"video\",\n",
        "                order=\"relevance\",\n",
        "                maxResults=50\n",
        "            )\n",
        "            response = request.execute()\n",
        "\n",
        "            for item in response['items']:\n",
        "                video_id = item['id']['videoId']\n",
        "                title = item['snippet']['title']\n",
        "\n",
        "                stats_request = self.youtube.videos().list(part=\"statistics\", id=video_id)\n",
        "                stats_response = stats_request.execute()\n",
        "\n",
        "                if stats_response['items']:\n",
        "                    stats = stats_response['items'][0]['statistics']\n",
        "                    comment_count = int(stats.get('commentCount', 0))\n",
        "\n",
        "                    if comment_count >= 5:  # Any video with 5+ comments\n",
        "                        print(f\"   üìπ Fallback: '{title[:50]}...' ({comment_count} comments)\")\n",
        "                        return {\n",
        "                            'video_id': video_id,\n",
        "                            'title': title,\n",
        "                            'comment_count': comment_count,\n",
        "                            'view_count': int(stats.get('viewCount', 0)),\n",
        "                            'engagement_score': comment_count,\n",
        "                            'period': 'all time'\n",
        "                        }\n",
        "\n",
        "            print(f\"   ‚ùå No videos with comments found for {channel_name}\")\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error getting video for {channel_name}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def extract_comments(self, video_id, video_title, target_comments=50):\n",
        "        \"\"\"Extract comments with flexible target.\"\"\"\n",
        "        print(f\"      üí¨ Extracting from: {video_title[:40]}...\")\n",
        "\n",
        "        comments = []\n",
        "        next_page_token = None\n",
        "        max_attempts = 5  # Prevent infinite loops\n",
        "        attempts = 0\n",
        "\n",
        "        try:\n",
        "            while len(comments) < target_comments and attempts < max_attempts:\n",
        "                attempts += 1\n",
        "\n",
        "                request = self.youtube.commentThreads().list(\n",
        "                    part=\"snippet,replies\",\n",
        "                    videoId=video_id,\n",
        "                    maxResults=100,\n",
        "                    pageToken=next_page_token,\n",
        "                    order=\"relevance\"\n",
        "                )\n",
        "                response = request.execute()\n",
        "\n",
        "                for item in response['items']:\n",
        "                    comment_data = item['snippet']['topLevelComment']['snippet']\n",
        "\n",
        "                    comment_info = {\n",
        "                        'comment_id': item['snippet']['topLevelComment']['id'],\n",
        "                        'text': comment_data['textDisplay'],\n",
        "                        'author': comment_data['authorDisplayName'],\n",
        "                        'likes': comment_data['likeCount'],\n",
        "                        'published': comment_data['publishedAt'],\n",
        "                        'video_id': video_id,\n",
        "                        'video_title': video_title\n",
        "                    }\n",
        "                    comments.append(comment_info)\n",
        "\n",
        "                    # Add replies (limit to 2 per comment to get more variety)\n",
        "                    if 'replies' in item and len(comments) < target_comments:\n",
        "                        for reply_item in item['replies']['comments'][:2]:\n",
        "                            if len(comments) >= target_comments:\n",
        "                                break\n",
        "                            reply_data = reply_item['snippet']\n",
        "                            reply_info = {\n",
        "                                'comment_id': reply_item['id'],\n",
        "                                'text': reply_data['textDisplay'],\n",
        "                                'author': reply_data['authorDisplayName'],\n",
        "                                'likes': reply_data['likeCount'],\n",
        "                                'published': reply_data['publishedAt'],\n",
        "                                'video_id': video_id,\n",
        "                                'video_title': video_title,\n",
        "                                'is_reply': True,\n",
        "                                'parent_id': item['snippet']['topLevelComment']['id']\n",
        "                            }\n",
        "                            comments.append(reply_info)\n",
        "\n",
        "                # Check for more pages\n",
        "                if 'nextPageToken' in response:\n",
        "                    next_page_token = response['nextPageToken']\n",
        "                else:\n",
        "                    break  # No more pages\n",
        "\n",
        "            print(f\"      ‚úÖ Extracted {len(comments)} comments\")\n",
        "            return comments\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"      ‚ùå Error extracting comments: {e}\")\n",
        "            return []\n",
        "\n",
        "    def filter_comments(self, comments):\n",
        "        \"\"\"Minimal filtering - remove obvious spam only.\"\"\"\n",
        "        print(\"üá∫üá¨ Applying minimal filtering...\")\n",
        "\n",
        "        filtered_comments = []\n",
        "        spam_patterns = ['first!', 'first comment', 'subscribe', 'follow me', 'sub 4 sub']\n",
        "\n",
        "        for comment in comments:\n",
        "            text = comment['text'].lower().strip()\n",
        "\n",
        "            # Skip very short comments\n",
        "            if len(text) < 3:\n",
        "                continue\n",
        "\n",
        "            # Skip spam\n",
        "            if any(pattern in text for pattern in spam_patterns):\n",
        "                continue\n",
        "\n",
        "            # Skip pure symbols/emoji\n",
        "            if re.match(r'^[\\W\\d_]+$', text):\n",
        "                continue\n",
        "\n",
        "            # Keep everything else\n",
        "            filtered_comments.append(comment)\n",
        "\n",
        "        removed_count = len(comments) - len(filtered_comments)\n",
        "        print(f\"      ‚úÖ Kept {len(filtered_comments)} comments\")\n",
        "        print(f\"      ‚ùå Filtered out {removed_count} spam comments\")\n",
        "\n",
        "        return filtered_comments\n",
        "\n",
        "    def get_next_version(self):\n",
        "        \"\"\"Get next version number.\"\"\"\n",
        "        existing_files = glob.glob(\"uganda_comments_v*.csv\")\n",
        "\n",
        "        if not existing_files:\n",
        "            return 1\n",
        "\n",
        "        versions = []\n",
        "        for filename in existing_files:\n",
        "            try:\n",
        "                version_str = filename.split('_v')[1].split('.')[0]\n",
        "                versions.append(int(version_str))\n",
        "            except (IndexError, ValueError):\n",
        "                continue\n",
        "\n",
        "        return max(versions) + 1 if versions else 1\n",
        "\n",
        "    def collect_all_comments(self):\n",
        "        \"\"\"Main collection function.\"\"\"\n",
        "        print(\"üá∫üá¨ UGANDA YOUTUBE COMMENT COLLECTION\")\n",
        "        print(\"=\" * 50)\n",
        "        print(\"Target: 2 channels per category, trending videos, 100+ comments each\")\n",
        "        print()\n",
        "\n",
        "        all_comments = []\n",
        "        channel_summary = []\n",
        "\n",
        "        for category, channels in self.target_channels.items():\n",
        "            print(f\"üìÇ Category: {category}\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            for channel_info in channels:\n",
        "                channel_name = channel_info['name']\n",
        "                search_term = channel_info['search_term']\n",
        "\n",
        "                print(f\"üîç Processing: {channel_name}\")\n",
        "\n",
        "                # Find channel\n",
        "                channel_id = self.find_channel_id(search_term)\n",
        "                if not channel_id:\n",
        "                    print(f\"   ‚ùå Could not find channel: {channel_name}\")\n",
        "                    continue\n",
        "\n",
        "                # Get trending video\n",
        "                trending_video = self.get_trending_video(channel_id, channel_name)\n",
        "                if not trending_video:\n",
        "                    continue\n",
        "\n",
        "                # Extract comments\n",
        "                comments = self.extract_comments(\n",
        "                    trending_video['video_id'],\n",
        "                    trending_video['title'],\n",
        "                    target_comments=50  # Fixed parameter name\n",
        "                )\n",
        "\n",
        "                if comments:\n",
        "                    # Filter comments\n",
        "                    filtered_comments = self.filter_comments(comments)\n",
        "\n",
        "                    # Add metadata to each comment\n",
        "                    for comment in filtered_comments:\n",
        "                        comment['category'] = category\n",
        "                        comment['channel_name'] = channel_name\n",
        "\n",
        "                    all_comments.extend(filtered_comments)\n",
        "\n",
        "                    # Add to summary\n",
        "                    channel_summary.append({\n",
        "                        'category': category,\n",
        "                        'channel_name': channel_name,\n",
        "                        'video_title': trending_video['title'],\n",
        "                        'video_views': trending_video['view_count'],\n",
        "                        'total_comments': len(comments),\n",
        "                        'filtered_comments': len(filtered_comments),\n",
        "                        'video_id': trending_video['video_id']\n",
        "                    })\n",
        "\n",
        "                    print(f\"   ‚úÖ {len(filtered_comments)} quality comments collected\")\n",
        "                else:\n",
        "                    print(f\"   ‚ùå No comments collected from {channel_name}\")\n",
        "\n",
        "                print()\n",
        "                time.sleep(1)  # Rate limiting\n",
        "\n",
        "            print()\n",
        "\n",
        "        # Save results\n",
        "        self.save_results(all_comments, channel_summary)\n",
        "\n",
        "        return all_comments, channel_summary\n",
        "\n",
        "    def save_results(self, all_comments, channel_summary):\n",
        "        \"\"\"Save results with versioned naming.\"\"\"\n",
        "        version = self.get_next_version()\n",
        "\n",
        "        # Save comments\n",
        "        if all_comments:\n",
        "            df_comments = pd.DataFrame(all_comments)\n",
        "            comments_filename = f\"uganda_comments_v{version}.csv\"\n",
        "            df_comments.to_csv(comments_filename, index=False, encoding='utf-8')\n",
        "            print(f\"üíæ Saved {len(all_comments)} comments to: {comments_filename}\")\n",
        "\n",
        "        # Save summary\n",
        "        if channel_summary:\n",
        "            df_summary = pd.DataFrame(channel_summary)\n",
        "            summary_filename = f\"channel_summary_v{version}.csv\"\n",
        "            df_summary.to_csv(summary_filename, index=False, encoding='utf-8')\n",
        "            print(f\"üìä Saved summary to: {summary_filename}\")\n",
        "\n",
        "        # Save metadata\n",
        "        metadata = {\n",
        "            'version': version,\n",
        "            'collection_date': datetime.now().isoformat(),\n",
        "            'total_comments': len(all_comments),\n",
        "            'channels_processed': len(channel_summary),\n",
        "            'categories': list(self.target_channels.keys())\n",
        "        }\n",
        "\n",
        "        metadata_filename = f\"collection_metadata_v{version}.json\"\n",
        "        with open(metadata_filename, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "        print(f\"üìã Saved metadata to: {metadata_filename}\")\n",
        "\n",
        "        # Summary\n",
        "        print(f\"\\nüéâ COLLECTION COMPLETE!\")\n",
        "        print(\"=\" * 30)\n",
        "        print(f\"üìä Total comments: {len(all_comments)}\")\n",
        "        print(f\"üì∫ Channels processed: {len(channel_summary)}\")\n",
        "\n",
        "        if all_comments:\n",
        "            print(f\"\\nüìà Comments by category:\")\n",
        "            category_counts = {}\n",
        "            for comment in all_comments:\n",
        "                category = comment['category']\n",
        "                category_counts[category] = category_counts.get(category, 0) + 1\n",
        "\n",
        "            for category, count in category_counts.items():\n",
        "                print(f\"   {category}: {count} comments\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function.\"\"\"\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        YOUTUBE_API_KEY = userdata.get('YouTube_API_Key')\n",
        "    except:\n",
        "        YOUTUBE_API_KEY = \"your-youtube-api-key-here\"\n",
        "\n",
        "    if YOUTUBE_API_KEY == \"your-youtube-api-key-here\":\n",
        "        print(\"‚ùå Please set YOUTUBE_API_KEY in Colab secrets!\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        extractor = CleanUgandaCommentExtractor(YOUTUBE_API_KEY)\n",
        "        comments, summary = extractor.collect_all_comments()\n",
        "\n",
        "        print(f\"\\nüöÄ Ready for sentiment analysis!\")\n",
        "        return comments, summary\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Collection failed: {e}\")\n",
        "        return None, None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ehs308xyOgO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. GANDA GEMMA MODEL SETUP"
      ],
      "metadata": {
        "id": "qXa1p3dkPAXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the CraneAILabs Ganda Gemma model for sentiment analysis.\n",
        "\n",
        "class LugandaSentimentAnalyzer:\n",
        "    def __init__(self, model_name=\"CraneAILabs/ganda-gemma-1b\"):\n",
        "        print(\"üá∫üá¨ Loading Ganda Gemma model...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Load model and tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name,\n",
        "            token=HF_TOKEN if HF_TOKEN != \"your-hf-token-here\" else None,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            token=HF_TOKEN if HF_TOKEN != \"your-hf-token-here\" else None,\n",
        "            torch_dtype=torch.float32,\n",
        "            device_map=\"cpu\",\n",
        "            trust_remote_code=True,\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "\n",
        "        # Set pad token\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "        load_time = time.time() - start_time\n",
        "        print(f\"Ganda Gemma loaded in {load_time:.2f} seconds\")\n",
        "\n",
        "    def analyze_sentiment(self, comment):\n",
        "        \"\"\"Analyze sentiment of a Luganda comment.\"\"\"\n",
        "        # Clean comment\n",
        "        clean_comment = re.sub(r'<[^>]+>', ' ', comment).strip()\n",
        "        if len(clean_comment) < 5:\n",
        "            return \"kibi\"\n",
        "\n",
        "        # Create prompt\n",
        "        prompt = f\"\"\"Analyze the sentiment of this Luganda comment. If the comment expresses positive feelings, happiness, love, or praise, respond with \"kirungi\". If the comment expresses negative feelings, sadness, anger, or criticism, respond with \"kibi\". Respond with only one word.\n",
        "\n",
        "Comment: {clean_comment}\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Tokenize and generate\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=10,\n",
        "                    temperature=0.3,\n",
        "                    do_sample=True,\n",
        "                    top_p=0.7\n",
        "                )\n",
        "\n",
        "            # Extract sentiment\n",
        "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            response_clean = response[len(prompt):].strip().lower()\n",
        "\n",
        "            # Map to Ugandan labels\n",
        "            if 'kirungi' in response_clean or any(word in response_clean for word in ['positive', 'good', 'happy']):\n",
        "                return \"kirungi\"\n",
        "            else:\n",
        "                return \"kibi\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Analysis error: {e}\")\n",
        "            return \"kibi\"\n",
        "\n",
        "# Initialize the analyzer\n",
        "if HF_TOKEN != \"your-hf-token-here\":\n",
        "    analyzer = LugandaSentimentAnalyzer()\n",
        "    print(\"Sentiment analyzer ready!\")\n",
        "else:\n",
        "    print(\"Add your Hugging Face token to load Ganda Gemma\")"
      ],
      "metadata": {
        "id": "CtRyIjokOyCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. SENTIMENT ANALYSIS"
      ],
      "metadata": {
        "id": "KNkAczDkPYIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your collected data\n",
        "df_comments = pd.read_csv('uganda_comments_v1.csv')\n",
        "print(f\"‚úÖ Loaded {len(df_comments)} comments from uganda_comments_v1.csv\")\n",
        "\n",
        "# Show a quick preview\n",
        "print(f\"üìä Comments by category:\")\n",
        "print(df_comments['category'].value_counts())"
      ],
      "metadata": {
        "id": "anpxFdaua2e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_comments.tail(5)"
      ],
      "metadata": {
        "id": "84qIjw4CbDYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyze the sentiment of our Luganda comments using authentic Ugandan labels.\n",
        "\n",
        "#Labels\n",
        "#Kirungi = Positive sentiment (good, nice, happy)\n",
        "#Kibi = Negative sentiment (bad, sad, angry)\n",
        "\n",
        "\n",
        "# Test with sample comments first\n",
        "test_comments = [\n",
        "    \"Nsanyuse nnyo! Eddy Kenzo webale kutuwereza!\",  # Should be kirungi\n",
        "    \"Alina omutima omubi\",                           # Should be kibi\n",
        "    \"Enkola ya government emalamu amaanyi\",          # Shouild be kibi\n",
        "]\n",
        "\n",
        "if 'analyzer' in globals():\n",
        "    print(\"üß™ Testing sentiment analysis:\")\n",
        "    for comment in test_comments:\n",
        "        sentiment = analyzer.analyze_sentiment(comment)\n",
        "        emoji = \"üòä\" if sentiment == \"kirungi\" else \"üòû\"\n",
        "        print(f\"{emoji} \\\"{comment}\\\" ‚Üí {sentiment}\")\n",
        "\n",
        "    # Analyze all comments\n",
        "    print(f\"\\nAnalyzing {len(df_comments)} comments...\")\n",
        "\n",
        "    sentiments = []\n",
        "    for comment in df_comments['text']:\n",
        "        sentiment = analyzer.analyze_sentiment(comment)\n",
        "        sentiments.append(sentiment)\n",
        "\n",
        "    df_comments['sentiment'] = sentiments\n",
        "    print(\"Sentiment analysis complete!\")\n",
        "\n",
        "else:\n",
        "    print(\"Using sample sentiment data\")\n",
        "    # Sample data for demonstration\n",
        "    df_comments['sentiment'] = ['kirungi', 'kibi', 'kirungi', 'kibi', 'kirungi']"
      ],
      "metadata": {
        "id": "yOqgAGUUPVRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. RESULTS ANALYSIS"
      ],
      "metadata": {
        "id": "tjtnumb2PqsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_comments.query(\"sentiment == 'kibi'\").head(30)"
      ],
      "metadata": {
        "id": "ciUa08t50pxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SENTIMENT ANALYSIS SUMMARY\n"
      ],
      "metadata": {
        "id": "9zNaO6P9xeeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Professional summary of Luganda sentiment analysis results.\n",
        "Provides core statistics and sample examples without visual embellishments.\n",
        "\"\"\"\n",
        "def generate_professional_sentiment_summary(df_comments):\n",
        "    \"\"\"\n",
        "    Generate a professional summary of sentiment analysis results.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): DataFrame with 'sentiment' and 'text' columns\n",
        "\n",
        "    Returns:\n",
        "    dict: Summary statistics for further use\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate sentiment distribution\n",
        "    sentiment_counts = df_comments['sentiment'].value_counts()\n",
        "    total_comments = len(df_comments)\n",
        "\n",
        "    print(\"LUGANDA SENTIMENT ANALYSIS RESULTS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total Comments Analyzed: {total_comments:,}\")\n",
        "    print(f\"Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\")\n",
        "    print()\n",
        "\n",
        "    # Overall statistics\n",
        "    print(\"SENTIMENT DISTRIBUTION\")\n",
        "    print(\"-\" * 25)\n",
        "\n",
        "    summary_stats = {}\n",
        "    for sentiment, count in sentiment_counts.items():\n",
        "        percentage = (count / total_comments) * 100\n",
        "        label = \"Positive (Good)\" if sentiment == \"kirungi\" else \"Negative (Bad)\"\n",
        "\n",
        "        print(f\"{sentiment.capitalize()} ({label}):\")\n",
        "        print(f\"  Count: {count:,}\")\n",
        "        print(f\"  Percentage: {percentage:.1f}%\")\n",
        "        print()\n",
        "\n",
        "        summary_stats[sentiment] = {\n",
        "            'count': count,\n",
        "            'percentage': percentage,\n",
        "            'label': label\n",
        "        }\n",
        "\n",
        "    # Calculate and interpret ratio\n",
        "    kirungi_count = sentiment_counts.get('kirungi', 0)\n",
        "    kibi_count = sentiment_counts.get('kibi', 0)\n",
        "\n",
        "    if kirungi_count > 0 and kibi_count > 0:\n",
        "        ratio = kirungi_count / kibi_count\n",
        "        print(\"SENTIMENT RATIO ANALYSIS\")\n",
        "        print(\"-\" * 25)\n",
        "        print(f\"Kirungi:Kibi Ratio: {ratio:.2f}:1\")\n",
        "\n",
        "        # Professional interpretation\n",
        "        if ratio > 2:\n",
        "            interpretation = \"Very positive audience reaction\"\n",
        "            recommendation = \"Strong positive engagement indicates high content quality and audience satisfaction.\"\n",
        "        elif ratio > 1:\n",
        "            interpretation = \"Generally positive audience reaction\"\n",
        "            recommendation = \"Positive engagement with room for improvement in addressing negative feedback.\"\n",
        "        elif ratio > 0.5:\n",
        "            interpretation = \"Mixed audience reaction\"\n",
        "            recommendation = \"Balanced feedback suggests need for content strategy review.\"\n",
        "        else:\n",
        "            interpretation = \"Predominantly negative audience reaction\"\n",
        "            recommendation = \"Significant negative feedback requires immediate content strategy reassessment.\"\n",
        "\n",
        "        print(f\"Interpretation: {interpretation}\")\n",
        "        print(f\"Recommendation: {recommendation}\")\n",
        "        print()\n",
        "\n",
        "        summary_stats['ratio'] = ratio\n",
        "        summary_stats['interpretation'] = interpretation\n",
        "        summary_stats['recommendation'] = recommendation\n",
        "\n",
        "    # Sample examples\n",
        "    print(\"REPRESENTATIVE EXAMPLES\")\n",
        "    print(\"-\" * 25)\n",
        "\n",
        "    for sentiment in ['kirungi', 'kibi']:\n",
        "        if sentiment in sentiment_counts:\n",
        "            samples = df_comments[df_comments['sentiment'] == sentiment].head(3)\n",
        "            label = \"POSITIVE (Kirungi)\" if sentiment == \"kirungi\" else \"NEGATIVE (Kibi)\"\n",
        "\n",
        "            print(f\"\\n{label} Examples:\")\n",
        "\n",
        "            for i, (_, row) in enumerate(samples.iterrows(), 1):\n",
        "                comment = row['text']\n",
        "                # Truncate long comments professionally\n",
        "                display_comment = comment if len(comment) <= 80 else comment[:77] + \"...\"\n",
        "                likes = row.get('likes', 0)\n",
        "\n",
        "                print(f\"  {i}. \\\"{display_comment}\\\"\")\n",
        "                print(f\"     Engagement: {likes} likes\")\n",
        "                if i < len(samples):\n",
        "                    print()\n",
        "\n",
        "    # Summary statistics for return\n",
        "    summary_stats['total_comments'] = total_comments\n",
        "    summary_stats['analysis_timestamp'] = pd.Timestamp.now()\n",
        "\n",
        "    return summary_stats\n",
        "\n",
        "def display_key_metrics(summary_stats):\n",
        "    \"\"\"\n",
        "    Display key metrics in a clean format.\n",
        "\n",
        "    Parameters:\n",
        "    summary_stats (dict): Summary statistics from generate_professional_sentiment_summary\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"KEY PERFORMANCE INDICATORS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    total = summary_stats['total_comments']\n",
        "    kirungi_pct = summary_stats.get('kirungi', {}).get('percentage', 0)\n",
        "    kibi_pct = summary_stats.get('kibi', {}).get('percentage', 0)\n",
        "    ratio = summary_stats.get('ratio', 0)\n",
        "\n",
        "    print(f\"Sample Size:           {total:,} comments\")\n",
        "    print(f\"Positive Rate:         {kirungi_pct:.1f}%\")\n",
        "    print(f\"Negative Rate:         {kibi_pct:.1f}%\")\n",
        "    print(f\"Positivity Ratio:      {ratio:.2f}:1\")\n",
        "\n",
        "    # Quality assessment\n",
        "    if kirungi_pct >= 60:\n",
        "        quality_score = \"Excellent\"\n",
        "    elif kirungi_pct >= 50:\n",
        "        quality_score = \"Good\"\n",
        "    elif kirungi_pct >= 40:\n",
        "        quality_score = \"Fair\"\n",
        "    else:\n",
        "        quality_score = \"Poor\"\n",
        "\n",
        "    print(f\"Content Quality:       {quality_score}\")\n",
        "\n",
        "    # Statistical confidence (basic)\n",
        "    confidence_level = \"High\" if total >= 500 else (\"Medium\" if total >= 100 else \"Low\")\n",
        "    print(f\"Statistical Confidence: {confidence_level}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the professional sentiment summary.\n",
        "    Assumes df_comments exists in the global scope.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if df_comments exists\n",
        "    try:\n",
        "        # This will be your loaded DataFrame\n",
        "        global df_comments\n",
        "\n",
        "        # Generate professional summary\n",
        "        stats = generate_professional_sentiment_summary(df_comments)\n",
        "\n",
        "        # Display key metrics\n",
        "        display_key_metrics(stats)\n",
        "\n",
        "        return stats\n",
        "\n",
        "    except NameError:\n",
        "        print(\"Error: df_comments not found. Please load your sentiment analysis data first.\")\n",
        "        print(\"Example: df_comments = pd.read_csv('uganda_comments_v1_with_sentiment.csv')\")\n",
        "        return None\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming df_comments is loaded\n",
        "    summary_statistics = main()"
      ],
      "metadata": {
        "id": "NhpI8LX9PtrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CATEGORY-BASED ANALYSIS"
      ],
      "metadata": {
        "id": "F34GerGrxn5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Analyzes sentiment patterns across different content categories, channels, and video types.\n",
        "Provides insights for content creators and stakeholders about audience preferences.\n",
        "\"\"\"\n",
        "def analyze_sentiment_by_category(df_comments):\n",
        "    \"\"\"\n",
        "    Analyze sentiment distribution across content categories.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): DataFrame with 'sentiment', 'category' columns\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Category analysis results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"SENTIMENT ANALYSIS BY CONTENT CATEGORY\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Group by category and sentiment\n",
        "    category_sentiment = df_comments.groupby(['category', 'sentiment']).size().unstack(fill_value=0)\n",
        "\n",
        "    # Calculate percentages and totals\n",
        "    category_analysis = pd.DataFrame()\n",
        "    category_analysis['Total_Comments'] = category_sentiment.sum(axis=1)\n",
        "    category_analysis['Kirungi_Count'] = category_sentiment.get('kirungi', 0)\n",
        "    category_analysis['Kibi_Count'] = category_sentiment.get('kibi', 0)\n",
        "    category_analysis['Kirungi_Percentage'] = (category_analysis['Kirungi_Count'] / category_analysis['Total_Comments'] * 100).round(1)\n",
        "    category_analysis['Kibi_Percentage'] = (category_analysis['Kibi_Count'] / category_analysis['Total_Comments'] * 100).round(1)\n",
        "    category_analysis['Positivity_Ratio'] = (category_analysis['Kirungi_Count'] / category_analysis['Kibi_Count'].replace(0, 1)).round(2)\n",
        "\n",
        "    # Sort by positivity ratio (best performing first)\n",
        "    category_analysis = category_analysis.sort_values('Positivity_Ratio', ascending=False)\n",
        "\n",
        "    print(\"CATEGORY PERFORMANCE RANKING\")\n",
        "    print(\"-\" * 35)\n",
        "    print(f\"{'Category':<25} {'Total':<8} {'Positive':<10} {'Negative':<10} {'Ratio':<8}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for category, row in category_analysis.iterrows():\n",
        "        print(f\"{category:<25} {row['Total_Comments']:<8} \"\n",
        "              f\"{row['Kirungi_Percentage']:>6.1f}% {row['Kibi_Percentage']:>9.1f}% \"\n",
        "              f\"{row['Positivity_Ratio']:>7.2f}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Category insights\n",
        "    print(\"CATEGORY INSIGHTS\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    best_category = category_analysis.index[0]\n",
        "    worst_category = category_analysis.index[-1]\n",
        "    most_commented = category_analysis.loc[category_analysis['Total_Comments'].idxmax()].name\n",
        "\n",
        "    print(f\"Best Performing Category:  {best_category}\")\n",
        "    print(f\"  Positivity Ratio: {category_analysis.loc[best_category, 'Positivity_Ratio']:.2f}:1\")\n",
        "    print(f\"  Positive Rate: {category_analysis.loc[best_category, 'Kirungi_Percentage']:.1f}%\")\n",
        "    print()\n",
        "\n",
        "    print(f\"Most Commented Category:   {most_commented}\")\n",
        "    print(f\"  Total Comments: {category_analysis.loc[most_commented, 'Total_Comments']:,}\")\n",
        "    print(f\"  Engagement Level: High\")\n",
        "    print()\n",
        "\n",
        "    print(f\"Needs Improvement:         {worst_category}\")\n",
        "    print(f\"  Positivity Ratio: {category_analysis.loc[worst_category, 'Positivity_Ratio']:.2f}:1\")\n",
        "    print(f\"  Negative Rate: {category_analysis.loc[worst_category, 'Kibi_Percentage']:.1f}%\")\n",
        "    print()\n",
        "\n",
        "    return category_analysis\n",
        "\n",
        "def analyze_sentiment_by_channel(df_comments):\n",
        "    \"\"\"\n",
        "    Analyze sentiment distribution across different channels.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): DataFrame with 'sentiment', 'channel_name' columns\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Channel analysis results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"SENTIMENT ANALYSIS BY CHANNEL\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Group by channel and sentiment\n",
        "    channel_sentiment = df_comments.groupby(['channel_name', 'sentiment']).size().unstack(fill_value=0)\n",
        "\n",
        "    # Calculate metrics\n",
        "    channel_analysis = pd.DataFrame()\n",
        "    channel_analysis['Total_Comments'] = channel_sentiment.sum(axis=1)\n",
        "    channel_analysis['Kirungi_Count'] = channel_sentiment.get('kirungi', 0)\n",
        "    channel_analysis['Kibi_Count'] = channel_sentiment.get('kibi', 0)\n",
        "    channel_analysis['Kirungi_Percentage'] = (channel_analysis['Kirungi_Count'] / channel_analysis['Total_Comments'] * 100).round(1)\n",
        "    channel_analysis['Positivity_Ratio'] = (channel_analysis['Kirungi_Count'] / channel_analysis['Kibi_Count'].replace(0, 1)).round(2)\n",
        "\n",
        "    # Sort by total comments (most engaging first)\n",
        "    channel_analysis = channel_analysis.sort_values('Total_Comments', ascending=False)\n",
        "\n",
        "    print(\"CHANNEL PERFORMANCE OVERVIEW\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"{'Channel':<20} {'Comments':<10} {'Positive%':<10} {'Ratio':<8}\")\n",
        "    print(\"-\" * 55)\n",
        "\n",
        "    for channel, row in channel_analysis.iterrows():\n",
        "        print(f\"{channel:<20} {row['Total_Comments']:<10} \"\n",
        "              f\"{row['Kirungi_Percentage']:>7.1f}% {row['Positivity_Ratio']:>9.2f}\")\n",
        "\n",
        "    # Channel insights\n",
        "    print(\"\\nCHANNEL INSIGHTS\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    top_engagement = channel_analysis.index[0]\n",
        "    top_positivity = channel_analysis.loc[channel_analysis['Positivity_Ratio'].idxmax()].name\n",
        "\n",
        "    print(f\"Highest Engagement:        {top_engagement}\")\n",
        "    print(f\"  Total Comments: {channel_analysis.loc[top_engagement, 'Total_Comments']:,}\")\n",
        "    print(f\"  Positive Rate: {channel_analysis.loc[top_engagement, 'Kirungi_Percentage']:.1f}%\")\n",
        "    print()\n",
        "\n",
        "    print(f\"Most Positive Channel:     {top_positivity}\")\n",
        "    print(f\"  Positivity Ratio: {channel_analysis.loc[top_positivity, 'Positivity_Ratio']:.2f}:1\")\n",
        "    print(f\"  Comments: {channel_analysis.loc[top_positivity, 'Total_Comments']:,}\")\n",
        "    print()\n",
        "\n",
        "    return channel_analysis\n",
        "\n",
        "def generate_content_strategy_recommendations(category_analysis, channel_analysis):\n",
        "    \"\"\"\n",
        "    Generate actionable recommendations based on category and channel analysis.\n",
        "\n",
        "    Parameters:\n",
        "    category_analysis (pd.DataFrame): Results from analyze_sentiment_by_category\n",
        "    channel_analysis (pd.DataFrame): Results from analyze_sentiment_by_channel\n",
        "\n",
        "    Returns:\n",
        "    dict: Structured recommendations\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"CONTENT STRATEGY RECOMMENDATIONS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    recommendations = {\n",
        "        'high_priority': [],\n",
        "        'medium_priority': [],\n",
        "        'opportunities': []\n",
        "    }\n",
        "\n",
        "    # High Priority Recommendations\n",
        "    print(\"HIGH PRIORITY ACTIONS\")\n",
        "    print(\"-\" * 25)\n",
        "\n",
        "    # Find categories with low positivity\n",
        "    low_positive_categories = category_analysis[category_analysis['Kirungi_Percentage'] < 50]\n",
        "\n",
        "    if not low_positive_categories.empty:\n",
        "        for category in low_positive_categories.index:\n",
        "            pct = category_analysis.loc[category, 'Kirungi_Percentage']\n",
        "            rec = f\"Improve {category} content quality (currently {pct:.1f}% positive)\"\n",
        "            print(f\"‚Ä¢ {rec}\")\n",
        "            recommendations['high_priority'].append(rec)\n",
        "\n",
        "    # Find channels with high negative sentiment\n",
        "    high_negative_channels = channel_analysis[channel_analysis['Kirungi_Percentage'] < 45]\n",
        "\n",
        "    if not high_negative_channels.empty:\n",
        "        for channel in high_negative_channels.index:\n",
        "            pct = channel_analysis.loc[channel, 'Kirungi_Percentage']\n",
        "            rec = f\"Review {channel} content strategy ({pct:.1f}% positive rate)\"\n",
        "            print(f\"‚Ä¢ {rec}\")\n",
        "            recommendations['high_priority'].append(rec)\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Medium Priority Recommendations\n",
        "    print(\"MEDIUM PRIORITY ACTIONS\")\n",
        "    print(\"-\" * 27)\n",
        "\n",
        "    # Categories with good but improvable performance\n",
        "    medium_categories = category_analysis[\n",
        "        (category_analysis['Kirungi_Percentage'] >= 50) &\n",
        "        (category_analysis['Kirungi_Percentage'] < 70)\n",
        "    ]\n",
        "\n",
        "    for category in medium_categories.index:\n",
        "        pct = category_analysis.loc[category, 'Kirungi_Percentage']\n",
        "        rec = f\"Optimize {category} content to reach 70%+ positive rate\"\n",
        "        print(f\"‚Ä¢ {rec}\")\n",
        "        recommendations['medium_priority'].append(rec)\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Opportunities\n",
        "    print(\"GROWTH OPPORTUNITIES\")\n",
        "    print(\"-\" * 25)\n",
        "\n",
        "    # Best performing categories to expand\n",
        "    top_categories = category_analysis.head(2)\n",
        "\n",
        "    for category in top_categories.index:\n",
        "        ratio = category_analysis.loc[category, 'Positivity_Ratio']\n",
        "        rec = f\"Expand {category} content production (strong {ratio:.2f}:1 ratio)\"\n",
        "        print(f\"‚Ä¢ {rec}\")\n",
        "        recommendations['opportunities'].append(rec)\n",
        "\n",
        "    # High engagement channels\n",
        "    top_channels = channel_analysis.head(2)\n",
        "\n",
        "    for channel in top_channels.index:\n",
        "        comments = channel_analysis.loc[channel, 'Total_Comments']\n",
        "        rec = f\"Collaborate more with {channel} (high engagement: {comments:,} comments)\"\n",
        "        print(f\"‚Ä¢ {rec}\")\n",
        "        recommendations['opportunities'].append(rec)\n",
        "\n",
        "    print()\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "def calculate_category_benchmarks(category_analysis):\n",
        "    \"\"\"\n",
        "    Calculate industry benchmarks and performance metrics.\n",
        "\n",
        "    Parameters:\n",
        "    category_analysis (pd.DataFrame): Category analysis results\n",
        "\n",
        "    Returns:\n",
        "    dict: Benchmark metrics\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"PERFORMANCE BENCHMARKS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Calculate benchmarks\n",
        "    avg_positivity = category_analysis['Kirungi_Percentage'].mean()\n",
        "    median_positivity = category_analysis['Kirungi_Percentage'].median()\n",
        "    avg_engagement = category_analysis['Total_Comments'].mean()\n",
        "\n",
        "    benchmarks = {\n",
        "        'average_positivity': avg_positivity,\n",
        "        'median_positivity': median_positivity,\n",
        "        'average_engagement': avg_engagement,\n",
        "        'excellence_threshold': avg_positivity + category_analysis['Kirungi_Percentage'].std(),\n",
        "        'improvement_threshold': avg_positivity - category_analysis['Kirungi_Percentage'].std()\n",
        "    }\n",
        "\n",
        "    print(\"UGANDA CONTENT BENCHMARKS\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Average Positivity Rate:    {avg_positivity:.1f}%\")\n",
        "    print(f\"Median Positivity Rate:     {median_positivity:.1f}%\")\n",
        "    print(f\"Average Comments per Video: {avg_engagement:.0f}\")\n",
        "    print(f\"Excellence Threshold:       {benchmarks['excellence_threshold']:.1f}%\")\n",
        "    print(f\"Improvement Needed Below:   {benchmarks['improvement_threshold']:.1f}%\")\n",
        "    print()\n",
        "\n",
        "    # Performance classification\n",
        "    print(\"CATEGORY PERFORMANCE CLASSIFICATION\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for category, row in category_analysis.iterrows():\n",
        "        pct = row['Kirungi_Percentage']\n",
        "\n",
        "        if pct >= benchmarks['excellence_threshold']:\n",
        "            status = \"Excellent\"\n",
        "        elif pct >= avg_positivity:\n",
        "            status = \"Above Average\"\n",
        "        elif pct >= benchmarks['improvement_threshold']:\n",
        "            status = \"Below Average\"\n",
        "        else:\n",
        "            status = \"Needs Improvement\"\n",
        "\n",
        "        print(f\"{category:<25} {status}\")\n",
        "\n",
        "    return benchmarks\n",
        "\n",
        "def run_category_analysis(df_comments):\n",
        "    \"\"\"\n",
        "    Run complete category-based sentiment analysis.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): DataFrame with sentiment analysis results\n",
        "\n",
        "    Returns:\n",
        "    dict: Complete analysis results\n",
        "    \"\"\"\n",
        "\n",
        "    # Run all analyses\n",
        "    category_results = analyze_sentiment_by_category(df_comments)\n",
        "    channel_results = analyze_sentiment_by_channel(df_comments)\n",
        "    recommendations = generate_content_strategy_recommendations(category_results, channel_results)\n",
        "    benchmarks = calculate_category_benchmarks(category_results)\n",
        "\n",
        "    # Return comprehensive results\n",
        "    return {\n",
        "        'category_analysis': category_results,\n",
        "        'channel_analysis': channel_results,\n",
        "        'recommendations': recommendations,\n",
        "        'benchmarks': benchmarks\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming df_comments is loaded with sentiment analysis\n",
        "    try:\n",
        "        # This will use your loaded DataFrame\n",
        "        analysis_results = run_category_analysis(df_comments)\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"CATEGORY ANALYSIS COMPLETE\")\n",
        "        print(\"All results stored in analysis_results dictionary\")\n",
        "\n",
        "    except NameError:\n",
        "        print(\"Error: df_comments not found. Please load your sentiment analysis data first.\")\n",
        "        print(\"Example: df_comments = pd.read_csv('uganda_comments_v1_with_sentiment.csv')\")"
      ],
      "metadata": {
        "id": "iyGjw_RVi_oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ENGAGEMENT ANALYSIS\n"
      ],
      "metadata": {
        "id": "Qdyanh-IyCto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Analyzes engagement patterns in Luganda comments including likes correlation,\n",
        "comment length patterns, and identifies most engaging content.\n",
        "\"\"\"\n",
        "def analyze_likes_sentiment_correlation(df_comments):\n",
        "    \"\"\"\n",
        "    Analyze correlation between likes and sentiment patterns.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): DataFrame with 'sentiment', 'likes' columns\n",
        "\n",
        "    Returns:\n",
        "    dict: Correlation analysis results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"ENGAGEMENT VS SENTIMENT CORRELATION ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Basic statistics\n",
        "    total_likes = df_comments['likes'].sum()\n",
        "    avg_likes_overall = df_comments['likes'].mean()\n",
        "\n",
        "    # Sentiment-based like analysis\n",
        "    sentiment_engagement = df_comments.groupby('sentiment').agg({\n",
        "        'likes': ['count', 'sum', 'mean', 'median', 'std'],\n",
        "        'text': 'count'\n",
        "    }).round(2)\n",
        "\n",
        "    sentiment_engagement.columns = ['Comment_Count', 'Total_Likes', 'Avg_Likes', 'Median_Likes', 'Std_Likes', 'Text_Count']\n",
        "\n",
        "    print(\"SENTIMENT ENGAGEMENT BREAKDOWN\")\n",
        "    print(\"-\" * 35)\n",
        "    print(f\"{'Sentiment':<12} {'Comments':<10} {'Total Likes':<12} {'Avg Likes':<10} {'Median':<8}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for sentiment in sentiment_engagement.index:\n",
        "        count = sentiment_engagement.loc[sentiment, 'Comment_Count']\n",
        "        total = sentiment_engagement.loc[sentiment, 'Total_Likes']\n",
        "        avg = sentiment_engagement.loc[sentiment, 'Avg_Likes']\n",
        "        median = sentiment_engagement.loc[sentiment, 'Median_Likes']\n",
        "\n",
        "        label = \"Positive\" if sentiment == \"kirungi\" else \"Negative\"\n",
        "        print(f\"{label:<12} {count:<10} {total:<12} {avg:<10.1f} {median:<8.1f}\")\n",
        "\n",
        "        results[sentiment] = {\n",
        "            'count': count,\n",
        "            'total_likes': total,\n",
        "            'avg_likes': avg,\n",
        "            'median_likes': median\n",
        "        }\n",
        "\n",
        "    # Calculate engagement ratios\n",
        "    kirungi_avg = results.get('kirungi', {}).get('avg_likes', 0)\n",
        "    kibi_avg = results.get('kibi', {}).get('avg_likes', 0)\n",
        "\n",
        "    if kibi_avg > 0:\n",
        "        engagement_ratio = kirungi_avg / kibi_avg\n",
        "    else:\n",
        "        engagement_ratio = float('inf') if kirungi_avg > 0 else 1\n",
        "\n",
        "    print(f\"\\nENGAGEMENT INSIGHTS\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"Total Platform Likes: {total_likes:,}\")\n",
        "    print(f\"Average Likes per Comment: {avg_likes_overall:.1f}\")\n",
        "    print(f\"Positive vs Negative Engagement Ratio: {engagement_ratio:.2f}:1\")\n",
        "\n",
        "    # Statistical correlation\n",
        "    # Convert sentiment to numeric for correlation\n",
        "    df_numeric = df_comments.copy()\n",
        "    df_numeric['sentiment_numeric'] = df_numeric['sentiment'].map({'kirungi': 1, 'kibi': 0})\n",
        "\n",
        "    if len(df_numeric) > 10:  # Need sufficient data for correlation\n",
        "        correlation, p_value = pearsonr(df_numeric['sentiment_numeric'], df_numeric['likes'])\n",
        "\n",
        "        print(f\"Pearson Correlation (Sentiment-Likes): {correlation:.3f}\")\n",
        "        print(f\"Statistical Significance: {'Yes' if p_value < 0.05 else 'No'} (p={p_value:.3f})\")\n",
        "\n",
        "        if correlation > 0.1:\n",
        "            interpretation = \"Positive comments tend to receive more likes\"\n",
        "        elif correlation < -0.1:\n",
        "            interpretation = \"Negative comments tend to receive more likes\"\n",
        "        else:\n",
        "            interpretation = \"No significant correlation between sentiment and likes\"\n",
        "\n",
        "        print(f\"Interpretation: {interpretation}\")\n",
        "\n",
        "        results['correlation'] = {\n",
        "            'value': correlation,\n",
        "            'p_value': p_value,\n",
        "            'interpretation': interpretation\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "def analyze_comment_length_patterns(df_comments):\n",
        "    \"\"\"\n",
        "    Analyze comment length patterns and their relationship with engagement.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): DataFrame with comment data\n",
        "\n",
        "    Returns:\n",
        "    dict: Length analysis results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"COMMENT LENGTH ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Calculate comment lengths\n",
        "    df_comments['comment_length'] = df_comments['text'].str.len()\n",
        "    df_comments['word_count'] = df_comments['text'].str.split().str.len()\n",
        "\n",
        "    # Length categories\n",
        "    def categorize_length(length):\n",
        "        if length < 50:\n",
        "            return \"Short\"\n",
        "        elif length < 150:\n",
        "            return \"Medium\"\n",
        "        else:\n",
        "            return \"Long\"\n",
        "\n",
        "    df_comments['length_category'] = df_comments['comment_length'].apply(categorize_length)\n",
        "\n",
        "    # Analysis by length category\n",
        "    length_analysis = df_comments.groupby('length_category').agg({\n",
        "        'likes': ['count', 'mean', 'sum'],\n",
        "        'comment_length': 'mean',\n",
        "        'word_count': 'mean'\n",
        "    }).round(2)\n",
        "\n",
        "    length_analysis.columns = ['Comment_Count', 'Avg_Likes', 'Total_Likes', 'Avg_Char_Length', 'Avg_Word_Count']\n",
        "\n",
        "    print(\"LENGTH CATEGORY PERFORMANCE\")\n",
        "    print(\"-\" * 35)\n",
        "    print(f\"{'Category':<10} {'Count':<8} {'Avg Likes':<10} {'Avg Length':<12} {'Avg Words':<10}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    for category in ['Short', 'Medium', 'Long']:\n",
        "        if category in length_analysis.index:\n",
        "            row = length_analysis.loc[category]\n",
        "            print(f\"{category:<10} {row['Comment_Count']:<8} {row['Avg_Likes']:<10.1f} \"\n",
        "                  f\"{row['Avg_Char_Length']:<12.0f} {row['Avg_Word_Count']:<10.1f}\")\n",
        "\n",
        "    # Sentiment by length\n",
        "    print(\"\\nSENTIMENT DISTRIBUTION BY LENGTH\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    sentiment_length = pd.crosstab(df_comments['length_category'], df_comments['sentiment'], normalize='index') * 100\n",
        "\n",
        "    if 'kirungi' in sentiment_length.columns and 'kibi' in sentiment_length.columns:\n",
        "        print(f\"{'Category':<10} {'Positive %':<12} {'Negative %':<12}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        for category in sentiment_length.index:\n",
        "            pos = sentiment_length.loc[category, 'kirungi']\n",
        "            neg = sentiment_length.loc[category, 'kibi']\n",
        "            print(f\"{category:<10} {pos:<12.1f} {neg:<12.1f}\")\n",
        "\n",
        "    # Optimal length insights\n",
        "    print(f\"\\nLENGTH INSIGHTS\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    best_length_cat = length_analysis.loc[length_analysis['Avg_Likes'].idxmax()].name\n",
        "    most_common_cat = length_analysis.loc[length_analysis['Comment_Count'].idxmax()].name\n",
        "\n",
        "    print(f\"Most Engaging Length: {best_length_cat}\")\n",
        "    print(f\"Most Common Length: {most_common_cat}\")\n",
        "\n",
        "    avg_length = df_comments['comment_length'].mean()\n",
        "    avg_words = df_comments['word_count'].mean()\n",
        "\n",
        "    print(f\"Average Comment Length: {avg_length:.0f} characters\")\n",
        "    print(f\"Average Word Count: {avg_words:.1f} words\")\n",
        "\n",
        "    return {\n",
        "        'length_analysis': length_analysis,\n",
        "        'avg_length': avg_length,\n",
        "        'avg_words': avg_words,\n",
        "        'best_length_category': best_length_cat\n",
        "    }\n",
        "\n",
        "def identify_most_engaging_comments(df_comments, top_n=10):\n",
        "    \"\"\"\n",
        "    Identify and analyze the most engaging comments.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): DataFrame with comment data\n",
        "    top_n (int): Number of top comments to analyze\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Top engaging comments with analysis\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"MOST ENGAGING COMMENTS ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Sort by likes and get top comments\n",
        "    top_comments = df_comments.nlargest(top_n, 'likes').copy()\n",
        "\n",
        "    # Add engagement metrics\n",
        "    top_comments['text_preview'] = top_comments['text'].apply(\n",
        "        lambda x: x[:80] + \"...\" if len(x) > 80 else x\n",
        "    )\n",
        "\n",
        "    print(f\"TOP {top_n} MOST LIKED COMMENTS\")\n",
        "    print(\"-\" * 35)\n",
        "    print(f\"{'Rank':<5} {'Likes':<7} {'Sentiment':<10} {'Category':<20} {'Preview'}\")\n",
        "    print(\"-\" * 90)\n",
        "\n",
        "    for i, (_, comment) in enumerate(top_comments.iterrows(), 1):\n",
        "        likes = int(comment['likes'])\n",
        "        sentiment = \"Positive\" if comment['sentiment'] == 'kirungi' else \"Negative\"\n",
        "        category = comment.get('category', 'Unknown')[:18]\n",
        "        preview = comment['text_preview']\n",
        "\n",
        "        print(f\"{i:<5} {likes:<7} {sentiment:<10} {category:<20} {preview}\")\n",
        "\n",
        "    # Analyze patterns in top comments\n",
        "    print(f\"\\nTOP COMMENTS ANALYSIS\")\n",
        "    print(\"-\" * 25)\n",
        "\n",
        "    sentiment_dist = top_comments['sentiment'].value_counts()\n",
        "    category_dist = top_comments['category'].value_counts() if 'category' in top_comments.columns else pd.Series()\n",
        "\n",
        "    print(\"Sentiment Distribution:\")\n",
        "    for sentiment, count in sentiment_dist.items():\n",
        "        label = \"Positive\" if sentiment == 'kirungi' else \"Negative\"\n",
        "        pct = (count / len(top_comments)) * 100\n",
        "        print(f\"  {label}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "    if not category_dist.empty:\n",
        "        print(f\"\\nTop Categories:\")\n",
        "        for category, count in category_dist.head(3).items():\n",
        "            pct = (count / len(top_comments)) * 100\n",
        "            print(f\"  {category}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "    # Engagement benchmarks\n",
        "    min_likes_top10 = top_comments['likes'].min()\n",
        "    avg_likes_top10 = top_comments['likes'].mean()\n",
        "\n",
        "    print(f\"\\nEngagement Benchmarks:\")\n",
        "    print(f\"  Top 10 Minimum Likes: {min_likes_top10}\")\n",
        "    print(f\"  Top 10 Average Likes: {avg_likes_top10:.1f}\")\n",
        "    print(f\"  Highest Single Comment: {top_comments['likes'].max()} likes\")\n",
        "\n",
        "    return top_comments[['text', 'likes', 'sentiment', 'category', 'comment_length', 'word_count']].copy()\n",
        "\n",
        "def calculate_engagement_score(df_comments):\n",
        "    \"\"\"\n",
        "    Calculate comprehensive engagement scores for content analysis.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): DataFrame with comment data\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Engagement scores by category/channel\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"ENGAGEMENT SCORE CALCULATION\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Calculate engagement score components\n",
        "    # Score = (Weighted Likes + Comment Volume + Sentiment Bonus) / Normalization Factor\n",
        "\n",
        "    category_engagement = df_comments.groupby('category').agg({\n",
        "        'likes': ['sum', 'mean', 'count'],\n",
        "        'sentiment': lambda x: (x == 'kirungi').sum() / len(x) * 100,\n",
        "        'comment_length': 'mean'\n",
        "    }).round(2)\n",
        "\n",
        "    category_engagement.columns = ['Total_Likes', 'Avg_Likes', 'Comment_Count', 'Positivity_Rate', 'Avg_Length']\n",
        "\n",
        "    # Calculate engagement score\n",
        "    max_likes = category_engagement['Total_Likes'].max()\n",
        "    max_comments = category_engagement['Comment_Count'].max()\n",
        "\n",
        "    category_engagement['Engagement_Score'] = (\n",
        "        (category_engagement['Total_Likes'] / max_likes * 40) +  # 40% weight on total likes\n",
        "        (category_engagement['Comment_Count'] / max_comments * 30) +  # 30% weight on volume\n",
        "        (category_engagement['Positivity_Rate'] / 100 * 20) +  # 20% weight on positivity\n",
        "        (category_engagement['Avg_Likes'] / category_engagement['Avg_Likes'].max() * 10)  # 10% weight on avg likes\n",
        "    ).round(1)\n",
        "\n",
        "    # Sort by engagement score\n",
        "    category_engagement = category_engagement.sort_values('Engagement_Score', ascending=False)\n",
        "\n",
        "    print(\"CATEGORY ENGAGEMENT SCORES\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"{'Category':<25} {'Score':<8} {'Total Likes':<12} {'Comments':<10} {'Positivity'}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for category, row in category_engagement.iterrows():\n",
        "        score = row['Engagement_Score']\n",
        "        likes = int(row['Total_Likes'])\n",
        "        comments = int(row['Comment_Count'])\n",
        "        positivity = row['Positivity_Rate']\n",
        "\n",
        "        print(f\"{category:<25} {score:<8} {likes:<12} {comments:<10} {positivity:.1f}%\")\n",
        "\n",
        "    # Performance tiers\n",
        "    print(f\"\\nPERFORMANCE TIERS\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    for category, row in category_engagement.iterrows():\n",
        "        score = row['Engagement_Score']\n",
        "\n",
        "        if score >= 80:\n",
        "            tier = \"Excellent\"\n",
        "        elif score >= 60:\n",
        "            tier = \"Good\"\n",
        "        elif score >= 40:\n",
        "            tier = \"Average\"\n",
        "        else:\n",
        "            tier = \"Needs Improvement\"\n",
        "\n",
        "        print(f\"{category:<25} {tier}\")\n",
        "\n",
        "    return category_engagement\n",
        "\n",
        "def run_engagement_analysis(df_comments):\n",
        "    \"\"\"\n",
        "    Run complete engagement analysis on sentiment-analyzed comments.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): DataFrame with sentiment analysis results\n",
        "\n",
        "    Returns:\n",
        "    dict: Complete engagement analysis results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"STARTING COMPREHENSIVE ENGAGEMENT ANALYSIS\")\n",
        "    print(\"=\" * 55)\n",
        "    print(f\"Analyzing {len(df_comments)} comments with sentiment data\")\n",
        "    print()\n",
        "\n",
        "    # Run all engagement analyses\n",
        "    correlation_results = analyze_likes_sentiment_correlation(df_comments)\n",
        "    length_results = analyze_comment_length_patterns(df_comments)\n",
        "    top_comments = identify_most_engaging_comments(df_comments)\n",
        "    engagement_scores = calculate_engagement_score(df_comments)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"ENGAGEMENT ANALYSIS COMPLETE\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Key Takeaways:\")\n",
        "\n",
        "    # Generate key insights\n",
        "    if 'correlation' in correlation_results:\n",
        "        corr_value = correlation_results['correlation']['value']\n",
        "        if corr_value > 0.1:\n",
        "            print(\"‚Ä¢ Positive sentiment correlates with higher engagement\")\n",
        "        elif corr_value < -0.1:\n",
        "            print(\"‚Ä¢ Controversial content may drive engagement\")\n",
        "        else:\n",
        "            print(\"‚Ä¢ Sentiment and likes show no strong correlation\")\n",
        "\n",
        "    best_length = length_results['best_length_category']\n",
        "    print(f\"‚Ä¢ {best_length} comments generate highest engagement\")\n",
        "\n",
        "    top_category = engagement_scores.index[0]\n",
        "    print(f\"‚Ä¢ {top_category} leads overall engagement metrics\")\n",
        "\n",
        "    return {\n",
        "        'correlation_analysis': correlation_results,\n",
        "        'length_analysis': length_results,\n",
        "        'top_comments': top_comments,\n",
        "        'engagement_scores': engagement_scores\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This script should be run after sentiment analysis is complete\n",
        "    try:\n",
        "        # Run engagement analysis on your sentiment-analyzed data\n",
        "        engagement_results = run_engagement_analysis(df_comments)\n",
        "\n",
        "        print(\"\\nAll engagement analysis results stored in engagement_results dictionary\")\n",
        "        print(\"Available keys:\", list(engagement_results.keys()))\n",
        "\n",
        "    except NameError:\n",
        "        print(\"Error: df_comments not found. Please run sentiment analysis first.\")\n",
        "        print(\"Make sure df_comments has 'sentiment', 'likes', 'text', and 'category' columns\")"
      ],
      "metadata": {
        "id": "21qGJOnvjjMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CONTENT ANALYSIS"
      ],
      "metadata": {
        "id": "qXi3etTsyT5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Analyzes content patterns in Luganda comments including word frequency,\n",
        "language mixing patterns, and keyword extraction for content insights.\n",
        "\"\"\"\n",
        "\n",
        "def preprocess_luganda_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess Luganda text for analysis while preserving language mixing.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): Raw comment text\n",
        "\n",
        "    Returns:\n",
        "    str: Cleaned text\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Keep only letters, numbers, and basic punctuation\n",
        "    text = re.sub(r'[^\\w\\s\\'\\-]', ' ', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def analyze_word_frequency(df_comments, top_n=50):\n",
        "    \"\"\"\n",
        "    Analyze word frequency patterns in Luganda comments.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): DataFrame with text column\n",
        "    top_n (int): Number of top words to analyze\n",
        "\n",
        "    Returns:\n",
        "    dict: Word frequency analysis results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"LUGANDA WORD FREQUENCY ANALYSIS\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Common English/Luganda stop words to filter\n",
        "    stop_words = {\n",
        "        'wa', 'ku', 'mu', 'ba', 'ki', 'ka', 'ga', 'lu', 'bu', 'tu', 'ma',  # Luganda prefixes\n",
        "        'ne', 'era', 'naye', 'oba', 'ate', 'nga', 'bwe', 'gwe', 'ye',  # Luganda conjunctions\n",
        "        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',  # English\n",
        "        'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
        "        'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might',\n",
        "        'of', 'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during',\n",
        "        'this', 'that', 'these', 'those', 'i', 'me', 'my', 'myself', 'we', 'our',\n",
        "        'you', 'your', 'he', 'him', 'his', 'she', 'her', 'it', 'its', 'they'\n",
        "    }\n",
        "\n",
        "    # Combine all text and process\n",
        "    all_text = ' '.join(df_comments['text'].astype(str))\n",
        "    cleaned_text = preprocess_luganda_text(all_text)\n",
        "\n",
        "    # Split into words and filter\n",
        "    words = cleaned_text.split()\n",
        "    filtered_words = [word for word in words if len(word) > 2 and word not in stop_words]\n",
        "\n",
        "    # Count frequencies\n",
        "    word_freq = Counter(filtered_words)\n",
        "\n",
        "    print(f\"OVERALL WORD STATISTICS\")\n",
        "    print(\"-\" * 25)\n",
        "    print(f\"Total words processed: {len(words):,}\")\n",
        "    print(f\"Unique words found: {len(word_freq):,}\")\n",
        "    print(f\"Words after filtering: {len(filtered_words):,}\")\n",
        "    print()\n",
        "\n",
        "    # Top words overall\n",
        "    print(f\"TOP {min(top_n, 30)} MOST FREQUENT WORDS\")\n",
        "    print(\"-\" * 35)\n",
        "    print(f\"{'Rank':<5} {'Word':<20} {'Count':<8} {'% of Total'}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    total_filtered = len(filtered_words)\n",
        "    top_words = {}\n",
        "\n",
        "    for i, (word, count) in enumerate(word_freq.most_common(min(top_n, 30)), 1):\n",
        "        percentage = (count / total_filtered) * 100\n",
        "        print(f\"{i:<5} {word:<20} {count:<8} {percentage:.2f}%\")\n",
        "        top_words[word] = {'count': count, 'percentage': percentage}\n",
        "\n",
        "    # Sentiment-based word analysis\n",
        "    print(f\"\\nSENTIMENT-BASED WORD PATTERNS\")\n",
        "    print(\"-\" * 35)\n",
        "\n",
        "    sentiment_words = {}\n",
        "\n",
        "    for sentiment in ['kirungi', 'kibi']:\n",
        "        sentiment_text = ' '.join(\n",
        "            df_comments[df_comments['sentiment'] == sentiment]['text'].astype(str)\n",
        "        )\n",
        "        sentiment_cleaned = preprocess_luganda_text(sentiment_text)\n",
        "        sentiment_word_list = [word for word in sentiment_cleaned.split()\n",
        "                              if len(word) > 2 and word not in stop_words]\n",
        "        sentiment_freq = Counter(sentiment_word_list)\n",
        "\n",
        "        label = \"POSITIVE (Kirungi)\" if sentiment == 'kirungi' else \"NEGATIVE (Kibi)\"\n",
        "        print(f\"\\n{label} - Top 10 Words:\")\n",
        "\n",
        "        sentiment_words[sentiment] = {}\n",
        "        for i, (word, count) in enumerate(sentiment_freq.most_common(10), 1):\n",
        "            pct = (count / len(sentiment_word_list)) * 100 if sentiment_word_list else 0\n",
        "            print(f\"  {i:2}. {word:<15} ({count:3}, {pct:.1f}%)\")\n",
        "            sentiment_words[sentiment][word] = {'count': count, 'percentage': pct}\n",
        "\n",
        "    return {\n",
        "        'total_words': len(words),\n",
        "        'unique_words': len(word_freq),\n",
        "        'filtered_words': len(filtered_words),\n",
        "        'top_words': top_words,\n",
        "        'sentiment_words': sentiment_words,\n",
        "        'word_frequency': dict(word_freq.most_common(top_n))\n",
        "    }\n",
        "\n",
        "def analyze_language_mixing(df_comments):\n",
        "    \"\"\"\n",
        "    Analyze English-Luganda language mixing patterns.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): DataFrame with text column\n",
        "\n",
        "    Returns:\n",
        "    dict: Language mixing analysis results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"LANGUAGE MIXING ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Common English words that appear in mixed content\n",
        "    english_indicators = {\n",
        "        'the', 'and', 'is', 'are', 'was', 'were', 'have', 'has', 'had', 'do', 'does', 'did',\n",
        "        'will', 'would', 'can', 'could', 'should', 'may', 'might', 'must', 'shall',\n",
        "        'this', 'that', 'these', 'those', 'my', 'your', 'his', 'her', 'our', 'their',\n",
        "        'good', 'bad', 'nice', 'great', 'best', 'worst', 'love', 'like', 'hate',\n",
        "        'people', 'time', 'work', 'money', 'government', 'president', 'minister',\n",
        "        'uganda', 'kampala', 'thanks', 'thank', 'please', 'sorry', 'very', 'really',\n",
        "        'always', 'never', 'sometimes', 'maybe', 'because', 'but', 'so', 'also'\n",
        "    }\n",
        "\n",
        "    # Common Luganda words\n",
        "    luganda_indicators = {\n",
        "        'webale', 'nkwagala', 'ssebo', 'nnyabo', 'bambi', 'kale', 'laba', 'genda',\n",
        "        'jjuuza', 'nkuba', 'nsanyuse', 'ntya', 'manya', 'temanyi', 'kuba', 'olemwa',\n",
        "        'abantu', 'emyaka', 'omulimu', 'ssente', 'gavumenti', 'pulezidenti',\n",
        "        'minisita', 'uganda', 'kampala', 'ddala', 'nnyo', 'banange', 'lwaki',\n",
        "        'otya', 'bulungi', 'bubi', 'kirungi', 'kibi', 'katonda', 'yesu', 'kristo'\n",
        "    }\n",
        "\n",
        "    def classify_language_mixing(text):\n",
        "        \"\"\"Classify text by language mixing level.\"\"\"\n",
        "        cleaned = preprocess_luganda_text(text)\n",
        "        words = set(cleaned.split())\n",
        "\n",
        "        english_count = len(words.intersection(english_indicators))\n",
        "        luganda_count = len(words.intersection(luganda_indicators))\n",
        "        total_indicator_words = english_count + luganda_count\n",
        "\n",
        "        if total_indicator_words == 0:\n",
        "            return \"Unknown\"\n",
        "        elif english_count == 0:\n",
        "            return \"Pure Luganda\"\n",
        "        elif luganda_count == 0:\n",
        "            return \"Pure English\"\n",
        "        elif english_count > luganda_count:\n",
        "            return \"English-Heavy Mix\"\n",
        "        elif luganda_count > english_count:\n",
        "            return \"Luganda-Heavy Mix\"\n",
        "        else:\n",
        "            return \"Balanced Mix\"\n",
        "\n",
        "    # Classify all comments\n",
        "    df_comments['language_mix'] = df_comments['text'].apply(classify_language_mixing)\n",
        "\n",
        "    # Analyze distribution\n",
        "    mix_distribution = df_comments['language_mix'].value_counts()\n",
        "\n",
        "    print(\"LANGUAGE MIXING DISTRIBUTION\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"{'Category':<20} {'Count':<8} {'Percentage'}\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    total_comments = len(df_comments)\n",
        "    mix_analysis = {}\n",
        "\n",
        "    for category, count in mix_distribution.items():\n",
        "        percentage = (count / total_comments) * 100\n",
        "        print(f\"{category:<20} {count:<8} {percentage:.1f}%\")\n",
        "        mix_analysis[category] = {'count': count, 'percentage': percentage}\n",
        "\n",
        "    # Analyze mixing by sentiment\n",
        "    print(f\"\\nLANGUAGE MIXING BY SENTIMENT\")\n",
        "    print(\"-\" * 35)\n",
        "\n",
        "    mixing_sentiment = pd.crosstab(df_comments['language_mix'], df_comments['sentiment'], normalize='index') * 100\n",
        "\n",
        "    if 'kirungi' in mixing_sentiment.columns and 'kibi' in mixing_sentiment.columns:\n",
        "        print(f\"{'Language Category':<20} {'Positive %':<12} {'Negative %'}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for category in mixing_sentiment.index:\n",
        "            pos = mixing_sentiment.loc[category, 'kirungi']\n",
        "            neg = mixing_sentiment.loc[category, 'kibi']\n",
        "            print(f\"{category:<20} {pos:<12.1f} {neg:.1f}%\")\n",
        "\n",
        "    # Analyze mixing by category\n",
        "    if 'category' in df_comments.columns:\n",
        "        print(f\"\\nLANGUAGE MIXING BY CONTENT CATEGORY\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        category_mixing = pd.crosstab(df_comments['category'], df_comments['language_mix'], normalize='index') * 100\n",
        "\n",
        "        print(\"Top mixing patterns by content category:\")\n",
        "        for content_cat in category_mixing.index:\n",
        "            top_mix = category_mixing.loc[content_cat].idxmax()\n",
        "            top_pct = category_mixing.loc[content_cat].max()\n",
        "            print(f\"  {content_cat}: {top_mix} ({top_pct:.1f}%)\")\n",
        "\n",
        "    return {\n",
        "        'distribution': mix_analysis,\n",
        "        'total_comments': total_comments,\n",
        "        'mixing_sentiment_correlation': mixing_sentiment.to_dict() if 'kirungi' in mixing_sentiment.columns else {},\n",
        "        'dominant_pattern': mix_distribution.index[0]\n",
        "    }\n",
        "\n",
        "def extract_keywords_and_themes(df_comments):\n",
        "    \"\"\"\n",
        "    Extract meaningful keywords and themes from comments.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): DataFrame with text and category columns\n",
        "\n",
        "    Returns:\n",
        "    dict: Keywords and themes analysis\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"KEYWORD AND THEME EXTRACTION\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Define theme keywords in both languages\n",
        "    theme_keywords = {\n",
        "        'praise_appreciation': [\n",
        "            'webale', 'nkwagala', 'thanks', 'thank', 'love', 'asante', 'good', 'nice',\n",
        "            'bulungi', 'kirungi', 'nsanyuse', 'amazing', 'great', 'excellent', 'perfect'\n",
        "        ],\n",
        "        'criticism_complaints': [\n",
        "            'kibi', 'bubi', 'bad', 'worst', 'hate', 'angry', 'disappointed', 'terrible',\n",
        "            'horrible', 'disgusting', 'shame', 'ntya', 'olemwa', 'problem', 'issue'\n",
        "        ],\n",
        "        'politics_government': [\n",
        "            'museveni', 'government', 'gavumenti', 'president', 'pulezidenti', 'minister',\n",
        "                'minisita', 'parliament', 'palimenti', 'nup', 'nrm', 'bobi', 'wine', 'politics',\n",
        "            'election', 'vote', 'leader', 'mukulembeze', 'opposition', 'ruling', 'party'\n",
        "        ],\n",
        "        'entertainment_music': [\n",
        "            'music', 'muziki', 'song', 'oluyimba', 'dance', 'okuzina', 'concert', 'show',\n",
        "            'artist', 'musician', 'singer', 'band', 'album', 'video', 'youtube', 'comedy',\n",
        "            'comedian', 'laugh', 'funny', 'entertainment', 'perform', 'stage'\n",
        "        ],\n",
        "        'religion_spirituality': [\n",
        "            'katonda', 'god', 'yesu', 'jesus', 'kristo', 'christ', 'bible', 'church',\n",
        "            'kkanisa', 'pray', 'prayer', 'okusaba', 'pastor', 'reverend', 'faith',\n",
        "            'okukkiriza', 'blessing', 'mukama', 'lord', 'amen', 'hallelujah'\n",
        "        ],\n",
        "        'social_issues': [\n",
        "            'poverty', 'obwavu', 'corruption', 'obuli', 'unemployment', 'education',\n",
        "            'ebyenjigiriza', 'health', 'obulamu', 'hospital', 'ddwaliro', 'medicine',\n",
        "            'eddagala', 'transport', 'entambula', 'roads', 'enguudo', 'infrastructure'\n",
        "        ],\n",
        "        'personal_emotions': [\n",
        "            'happy', 'nsanyuse', 'sad', 'nkungubaga', 'excited', 'tired', 'nkoowa',\n",
        "            'proud', 'neegulumiza', 'worried', 'nfudde', 'confused', 'stressed',\n",
        "            'relaxed', 'motivated', 'inspired', 'grateful', 'blessed'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    def extract_themes_from_text(text):\n",
        "        \"\"\"Extract themes present in a text.\"\"\"\n",
        "        cleaned = preprocess_luganda_text(text)\n",
        "        words = set(cleaned.split())\n",
        "\n",
        "        themes_found = []\n",
        "        for theme, keywords in theme_keywords.items():\n",
        "            if any(keyword in words for keyword in keywords):\n",
        "                themes_found.append(theme)\n",
        "\n",
        "        return themes_found\n",
        "\n",
        "    # Extract themes for all comments\n",
        "    df_comments['themes'] = df_comments['text'].apply(extract_themes_from_text)\n",
        "\n",
        "    # Count theme occurrences\n",
        "    theme_counts = defaultdict(int)\n",
        "    for themes_list in df_comments['themes']:\n",
        "        for theme in themes_list:\n",
        "            theme_counts[theme] += 1\n",
        "\n",
        "    total_comments = len(df_comments)\n",
        "\n",
        "    print(\"THEME FREQUENCY ANALYSIS\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"{'Theme':<25} {'Count':<8} {'Percentage'}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    theme_analysis = {}\n",
        "    for theme, count in sorted(theme_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "        percentage = (count / total_comments) * 100\n",
        "        theme_name = theme.replace('_', ' ').title()\n",
        "        print(f\"{theme_name:<25} {count:<8} {percentage:.1f}%\")\n",
        "        theme_analysis[theme] = {'count': count, 'percentage': percentage}\n",
        "\n",
        "    # Analyze themes by sentiment\n",
        "    print(f\"\\nTHEME SENTIMENT ANALYSIS\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    theme_sentiment = {}\n",
        "\n",
        "    for theme in theme_counts.keys():\n",
        "        # Find comments containing this theme\n",
        "        theme_comments = df_comments[df_comments['themes'].apply(lambda x: theme in x)]\n",
        "\n",
        "        if len(theme_comments) > 0:\n",
        "            sentiment_dist = theme_comments['sentiment'].value_counts(normalize=True) * 100\n",
        "\n",
        "            kirungi_pct = sentiment_dist.get('kirungi', 0)\n",
        "            kibi_pct = sentiment_dist.get('kibi', 0)\n",
        "\n",
        "            theme_sentiment[theme] = {\n",
        "                'positive_pct': kirungi_pct,\n",
        "                'negative_pct': kibi_pct,\n",
        "                'total_comments': len(theme_comments)\n",
        "            }\n",
        "\n",
        "    print(f\"{'Theme':<25} {'Positive %':<12} {'Negative %':<12} {'Comments'}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    for theme, data in sorted(theme_sentiment.items(), key=lambda x: x[1]['positive_pct'], reverse=True):\n",
        "        theme_name = theme.replace('_', ' ').title()\n",
        "        pos_pct = data['positive_pct']\n",
        "        neg_pct = data['negative_pct']\n",
        "        comments = data['total_comments']\n",
        "        print(f\"{theme_name:<25} {pos_pct:<12.1f} {neg_pct:<12.1f} {comments}\")\n",
        "\n",
        "    # Analyze themes by category\n",
        "    if 'category' in df_comments.columns:\n",
        "        print(f\"\\nTHEME DISTRIBUTION BY CATEGORY\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        category_themes = {}\n",
        "        for category in df_comments['category'].unique():\n",
        "            cat_comments = df_comments[df_comments['category'] == category]\n",
        "            cat_theme_counts = defaultdict(int)\n",
        "\n",
        "            for themes_list in cat_comments['themes']:\n",
        "                for theme in themes_list:\n",
        "                    cat_theme_counts[theme] += 1\n",
        "\n",
        "            if cat_theme_counts:\n",
        "                top_theme = max(cat_theme_counts.items(), key=lambda x: x[1])\n",
        "                theme_name = top_theme[0].replace('_', ' ').title()\n",
        "                count = top_theme[1]\n",
        "                pct = (count / len(cat_comments)) * 100\n",
        "\n",
        "                print(f\"{category}: {theme_name} ({count} comments, {pct:.1f}%)\")\n",
        "                category_themes[category] = {\n",
        "                    'top_theme': top_theme[0],\n",
        "                    'count': count,\n",
        "                    'percentage': pct\n",
        "                }\n",
        "\n",
        "    return {\n",
        "        'theme_counts': dict(theme_counts),\n",
        "        'theme_analysis': theme_analysis,\n",
        "        'theme_sentiment': theme_sentiment,\n",
        "        'category_themes': category_themes if 'category' in df_comments.columns else {},\n",
        "        'total_comments_analyzed': total_comments\n",
        "    }\n",
        "\n",
        "def analyze_viral_content_patterns(df_comments, viral_threshold=None):\n",
        "    \"\"\"\n",
        "    Analyze patterns in viral content (high-engagement comments).\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): DataFrame with likes and text columns\n",
        "    viral_threshold (int): Minimum likes to be considered viral (auto-calculated if None)\n",
        "\n",
        "    Returns:\n",
        "    dict: Viral content analysis results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"VIRAL CONTENT PATTERN ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Calculate viral threshold if not provided\n",
        "    if viral_threshold is None:\n",
        "        likes_75th = df_comments['likes'].quantile(0.75)\n",
        "        likes_90th = df_comments['likes'].quantile(0.90)\n",
        "        viral_threshold = max(likes_75th, 10)  # At least 10 likes or 75th percentile\n",
        "\n",
        "    viral_comments = df_comments[df_comments['likes'] >= viral_threshold].copy()\n",
        "    regular_comments = df_comments[df_comments['likes'] < viral_threshold].copy()\n",
        "\n",
        "    print(f\"VIRAL CONTENT DEFINITION\")\n",
        "    print(\"-\" * 25)\n",
        "    print(f\"Viral Threshold: {viral_threshold} likes\")\n",
        "    print(f\"Viral Comments: {len(viral_comments)} ({len(viral_comments)/len(df_comments)*100:.1f}%)\")\n",
        "    print(f\"Regular Comments: {len(regular_comments)} ({len(regular_comments)/len(df_comments)*100:.1f}%)\")\n",
        "    print()\n",
        "\n",
        "    # Analyze viral content characteristics\n",
        "    viral_analysis = {}\n",
        "\n",
        "    # Length analysis\n",
        "    viral_avg_length = viral_comments['text'].str.len().mean() if len(viral_comments) > 0 else 0\n",
        "    regular_avg_length = regular_comments['text'].str.len().mean() if len(regular_comments) > 0 else 0\n",
        "\n",
        "    print(f\"VIRAL CONTENT CHARACTERISTICS\")\n",
        "    print(\"-\" * 35)\n",
        "    print(f\"Average Length - Viral: {viral_avg_length:.0f} characters\")\n",
        "    print(f\"Average Length - Regular: {regular_avg_length:.0f} characters\")\n",
        "    print(f\"Length Difference: {viral_avg_length - regular_avg_length:+.0f} characters\")\n",
        "    print()\n",
        "\n",
        "    # Sentiment analysis\n",
        "    if len(viral_comments) > 0 and len(regular_comments) > 0:\n",
        "        viral_sentiment = viral_comments['sentiment'].value_counts(normalize=True) * 100\n",
        "        regular_sentiment = regular_comments['sentiment'].value_counts(normalize=True) * 100\n",
        "\n",
        "        print(f\"SENTIMENT DISTRIBUTION\")\n",
        "        print(\"-\" * 25)\n",
        "        print(f\"{'Sentiment':<12} {'Viral %':<10} {'Regular %'}\")\n",
        "        print(\"-\" * 35)\n",
        "\n",
        "        for sentiment in ['kirungi', 'kibi']:\n",
        "            viral_pct = viral_sentiment.get(sentiment, 0)\n",
        "            regular_pct = regular_sentiment.get(sentiment, 0)\n",
        "            label = \"Positive\" if sentiment == 'kirungi' else \"Negative\"\n",
        "            print(f\"{label:<12} {viral_pct:<10.1f} {regular_pct:.1f}%\")\n",
        "\n",
        "    # Extract common patterns in viral content\n",
        "    if len(viral_comments) > 0:\n",
        "        viral_text = ' '.join(viral_comments['text'].astype(str))\n",
        "        viral_cleaned = preprocess_luganda_text(viral_text)\n",
        "        viral_words = [word for word in viral_cleaned.split() if len(word) > 3]\n",
        "        viral_word_freq = Counter(viral_words)\n",
        "\n",
        "        print(f\"\\nTOP VIRAL CONTENT WORDS\")\n",
        "        print(\"-\" * 25)\n",
        "\n",
        "        for i, (word, count) in enumerate(viral_word_freq.most_common(15), 1):\n",
        "            print(f\"{i:2}. {word:<15} ({count} times)\")\n",
        "\n",
        "    viral_analysis = {\n",
        "        'threshold': viral_threshold,\n",
        "        'viral_count': len(viral_comments),\n",
        "        'viral_percentage': len(viral_comments)/len(df_comments)*100 if len(df_comments) > 0 else 0,\n",
        "        'avg_length_viral': viral_avg_length,\n",
        "        'avg_length_regular': regular_avg_length,\n",
        "        'viral_sentiment': viral_sentiment.to_dict() if len(viral_comments) > 0 else {},\n",
        "        'top_viral_words': dict(viral_word_freq.most_common(20)) if len(viral_comments) > 0 else {}\n",
        "    }\n",
        "\n",
        "    return viral_analysis\n",
        "\n",
        "def analyze_viral_content_patterns(df_comments, viral_threshold=None):\n",
        "    \"\"\"\n",
        "    Analyze patterns in viral content (high-engagement comments).\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): DataFrame with likes and text columns\n",
        "    viral_threshold (int): Minimum likes to be considered viral (auto-calculated if None)\n",
        "\n",
        "    Returns:\n",
        "    dict: Viral content analysis results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"VIRAL CONTENT PATTERN ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Calculate viral threshold if not provided\n",
        "    if viral_threshold is None:\n",
        "        likes_75th = df_comments['likes'].quantile(0.75)\n",
        "        likes_90th = df_comments['likes'].quantile(0.90)\n",
        "        viral_threshold = max(likes_75th, 10)  # At least 10 likes or 75th percentile\n",
        "\n",
        "    viral_comments = df_comments[df_comments['likes'] >= viral_threshold].copy()\n",
        "    regular_comments = df_comments[df_comments['likes'] < viral_threshold].copy()\n",
        "\n",
        "    print(f\"VIRAL CONTENT DEFINITION\")\n",
        "    print(\"-\" * 25)\n",
        "    print(f\"Viral Threshold: {viral_threshold} likes\")\n",
        "    print(f\"Viral Comments: {len(viral_comments)} ({len(viral_comments)/len(df_comments)*100:.1f}%)\")\n",
        "    print(f\"Regular Comments: {len(regular_comments)} ({len(regular_comments)/len(df_comments)*100:.1f}%)\")\n",
        "    print()\n",
        "\n",
        "    # Analyze viral content characteristics\n",
        "    viral_analysis = {}\n",
        "\n",
        "    # Length analysis\n",
        "    viral_avg_length = viral_comments['text'].str.len().mean()\n",
        "    regular_avg_length = regular_comments['text'].str.len().mean()\n",
        "\n",
        "    print(f\"VIRAL CONTENT CHARACTERISTICS\")\n",
        "    print(\"-\" * 35)\n",
        "    print(f\"Average Length - Viral: {viral_avg_length:.0f} characters\")\n",
        "    print(f\"Average Length - Regular: {regular_avg_length:.0f} characters\")\n",
        "    print(f\"Length Difference: {viral_avg_length - regular_avg_length:+.0f} characters\")\n",
        "    print()\n",
        "\n",
        "    # Sentiment analysis\n",
        "    if len(viral_comments) > 0:\n",
        "        viral_sentiment = viral_comments['sentiment'].value_counts(normalize=True) * 100\n",
        "        regular_sentiment = regular_comments['sentiment'].value_counts(normalize=True) * 100\n",
        "\n",
        "        print(f\"SENTIMENT DISTRIBUTION\")\n",
        "        print(\"-\" * 25)\n",
        "        print(f\"{'Sentiment':<12} {'Viral %':<10} {'Regular %'}\")\n",
        "        print(\"-\" * 35)\n",
        "\n",
        "        for sentiment in ['kirungi', 'kibi']:\n",
        "            viral_pct = viral_sentiment.get(sentiment, 0)\n",
        "            regular_pct = regular_sentiment.get(sentiment, 0)\n",
        "            label = \"Positive\" if sentiment == 'kirungi' else \"Negative\"\n",
        "            print(f\"{label:<12} {viral_pct:<10.1f} {regular_pct:.1f}%\")\n",
        "\n",
        "    # Extract common patterns in viral content\n",
        "    if len(viral_comments) > 0:\n",
        "        viral_text = ' '.join(viral_comments['text'].astype(str))\n",
        "        viral_cleaned = preprocess_luganda_text(viral_text)\n",
        "        viral_words = [word for word in viral_cleaned.split() if len(word) > 3]\n",
        "        viral_word_freq = Counter(viral_words)\n",
        "\n",
        "        print(f\"\\nTOP VIRAL CONTENT WORDS\")\n",
        "        print(\"-\" * 25)\n",
        "\n",
        "        for i, (word, count) in enumerate(viral_word_freq.most_common(15), 1):\n",
        "            print(f\"{i:2}. {word:<15} ({count} times)\")\n",
        "\n",
        "    viral_analysis = {\n",
        "        'threshold': viral_threshold,\n",
        "        'viral_count': len(viral_comments),\n",
        "        'viral_percentage': len(viral_comments)/len(df_comments)*100,\n",
        "        'avg_length_viral': viral_avg_length,\n",
        "        'avg_length_regular': regular_avg_length,\n",
        "        'viral_sentiment': viral_sentiment.to_dict() if len(viral_comments) > 0 else {},\n",
        "        'top_viral_words': dict(viral_word_freq.most_common(20)) if len(viral_comments) > 0 else {}\n",
        "    }\n",
        "\n",
        "    return viral_analysis\n",
        "\n",
        "def run_content_analysis(df_comments):\n",
        "    \"\"\"\n",
        "    Run comprehensive content analysis on Luganda comments.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): DataFrame with sentiment analysis results\n",
        "\n",
        "    Returns:\n",
        "    dict: Complete content analysis results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"STARTING COMPREHENSIVE CONTENT ANALYSIS\")\n",
        "    print(\"=\" * 55)\n",
        "    print(f\"Analyzing {len(df_comments)} Luganda comments\")\n",
        "    print()\n",
        "\n",
        "    # Run all content analyses\n",
        "    word_frequency = analyze_word_frequency(df_comments)\n",
        "    language_mixing = analyze_language_mixing(df_comments)\n",
        "    themes_keywords = extract_keywords_and_themes(df_comments)\n",
        "    viral_patterns = analyze_viral_content_patterns(df_comments)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"CONTENT ANALYSIS COMPLETE\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Generate key insights\n",
        "    print(\"KEY CONTENT INSIGHTS:\")\n",
        "\n",
        "    # Language insights\n",
        "    dominant_mix = language_mixing['dominant_pattern']\n",
        "    print(f\"‚Ä¢ Dominant language pattern: {dominant_mix}\")\n",
        "\n",
        "    # Theme insights\n",
        "    if themes_keywords['theme_counts']:\n",
        "        top_theme = max(themes_keywords['theme_counts'].items(), key=lambda x: x[1])\n",
        "        theme_name = top_theme[0].replace('_', ' ').title()\n",
        "        print(f\"‚Ä¢ Most discussed theme: {theme_name} ({top_theme[1]} comments)\")\n",
        "\n",
        "    # Viral content insights\n",
        "    viral_pct = viral_patterns['viral_percentage']\n",
        "    print(f\"‚Ä¢ Viral content rate: {viral_pct:.1f}% of comments\")\n",
        "\n",
        "    # Word frequency insights\n",
        "    if word_frequency['top_words']:\n",
        "        top_word = list(word_frequency['top_words'].keys())[0]\n",
        "        print(f\"‚Ä¢ Most frequent word: '{top_word}'\")\n",
        "\n",
        "    return {\n",
        "        'word_frequency': word_frequency,\n",
        "        'language_mixing': language_mixing,\n",
        "        'themes_keywords': themes_keywords,\n",
        "        'viral_patterns': viral_patterns\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This script should be run after sentiment analysis is complete\n",
        "    try:\n",
        "        # Run content analysis on your sentiment-analyzed data\n",
        "        content_results = run_content_analysis(df_comments)\n",
        "\n",
        "        print(\"\\nAll content analysis results stored in content_results dictionary\")\n",
        "        print(\"Available keys:\", list(content_results.keys()))\n",
        "\n",
        "    except NameError:\n",
        "        print(\"Error: df_comments not found. Please run sentiment analysis first.\")\n",
        "        print(\"Make sure df_comments has 'sentiment', 'text', and 'category' columns\")"
      ],
      "metadata": {
        "id": "BQaGVsRYjjzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BUSINESS INTELLIGENCE DASHBOARD"
      ],
      "metadata": {
        "id": "8KaQhXIeyi3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creates comprehensive business intelligence dashboard with creator insights,\n",
        "strategic recommendations, and executive summary metrics for Ugandan content.\n",
        "\"\"\"\n",
        "\n",
        "def generate_executive_summary(df_comments, analysis_results=None):\n",
        "    \"\"\"\n",
        "    Generate executive-level summary of Ugandan content performance.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): Comments with sentiment analysis\n",
        "    analysis_results (dict): Results from previous analysis scripts\n",
        "\n",
        "    Returns:\n",
        "    dict: Executive summary metrics\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"UGANDA CONTENT INTELLIGENCE DASHBOARD\")\n",
        "    print(\"=\" * 55)\n",
        "    print(f\"Executive Summary | Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
        "    print(\"=\" * 55)\n",
        "\n",
        "    # Core metrics\n",
        "    total_comments = len(df_comments)\n",
        "    total_engagement = df_comments['likes'].sum()\n",
        "    avg_engagement = df_comments['likes'].mean()\n",
        "\n",
        "    # Sentiment metrics\n",
        "    sentiment_dist = df_comments['sentiment'].value_counts(normalize=True) * 100\n",
        "    positive_rate = sentiment_dist.get('kirungi', 0)\n",
        "    negative_rate = sentiment_dist.get('kibi', 0)\n",
        "\n",
        "    # Calculate business impact score\n",
        "    # Score based on volume, engagement, sentiment, and content diversity\n",
        "    categories = df_comments['category'].nunique() if 'category' in df_comments.columns else 1\n",
        "    channels = df_comments['channel_name'].nunique() if 'channel_name' in df_comments.columns else 1\n",
        "\n",
        "    # Business Impact Score (0-100)\n",
        "    volume_score = min((total_comments / 500) * 25, 25)  # Max 25 points for volume\n",
        "    engagement_score = min((avg_engagement / 10) * 25, 25)  # Max 25 points for engagement\n",
        "    sentiment_score = (positive_rate / 100) * 30  # Max 30 points for positivity\n",
        "    diversity_score = min((categories + channels) / 10 * 20, 20)  # Max 20 points for diversity\n",
        "\n",
        "    business_impact_score = volume_score + engagement_score + sentiment_score + diversity_score\n",
        "\n",
        "    print(\"üìä KEY PERFORMANCE INDICATORS\")\n",
        "    print(\"-\" * 35)\n",
        "    print(f\"Total Comments Analyzed:     {total_comments:,}\")\n",
        "    print(f\"Total Engagement (Likes):    {total_engagement:,}\")\n",
        "    print(f\"Average Engagement:          {avg_engagement:.1f} likes/comment\")\n",
        "    print(f\"Audience Sentiment:          {positive_rate:.1f}% Positive\")\n",
        "    print(f\"Content Categories:          {categories}\")\n",
        "    print(f\"Active Channels:             {channels}\")\n",
        "    print(f\"Business Impact Score:       {business_impact_score:.1f}/100\")\n",
        "    print()\n",
        "\n",
        "    # Performance assessment\n",
        "    if business_impact_score >= 80:\n",
        "        performance_grade = \"Excellent (A)\"\n",
        "        performance_color = \"üü¢\"\n",
        "        performance_action = \"Maintain current strategy\"\n",
        "    elif business_impact_score >= 65:\n",
        "        performance_grade = \"Good (B)\"\n",
        "        performance_color = \"üü°\"\n",
        "        performance_action = \"Optimize high-performing areas\"\n",
        "    elif business_impact_score >= 50:\n",
        "        performance_grade = \"Average (C)\"\n",
        "        performance_color = \"üü†\"\n",
        "        performance_action = \"Significant improvement needed\"\n",
        "    else:\n",
        "        performance_grade = \"Poor (D)\"\n",
        "        performance_color = \"üî¥\"\n",
        "        performance_action = \"Complete strategy overhaul required\"\n",
        "\n",
        "    print(f\"üìà OVERALL PERFORMANCE ASSESSMENT\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Grade: {performance_color} {performance_grade}\")\n",
        "    print(f\"Recommendation: {performance_action}\")\n",
        "    print()\n",
        "\n",
        "    return {\n",
        "        'total_comments': total_comments,\n",
        "        'total_engagement': total_engagement,\n",
        "        'avg_engagement': avg_engagement,\n",
        "        'positive_rate': positive_rate,\n",
        "        'negative_rate': negative_rate,\n",
        "        'business_impact_score': business_impact_score,\n",
        "        'performance_grade': performance_grade,\n",
        "        'performance_action': performance_action,\n",
        "        'categories': categories,\n",
        "        'channels': channels\n",
        "    }\n",
        "\n",
        "def analyze_creator_performance(df_comments):\n",
        "    \"\"\"\n",
        "    Analyze individual creator/channel performance with actionable insights.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): Comments with channel information\n",
        "\n",
        "    Returns:\n",
        "    dict: Creator performance analysis\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üé• CREATOR PERFORMANCE ANALYSIS\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    if 'channel_name' not in df_comments.columns:\n",
        "        print(\"Channel information not available for creator analysis\")\n",
        "        return {}\n",
        "\n",
        "    # Calculate creator metrics\n",
        "    creator_metrics = df_comments.groupby('channel_name').agg({\n",
        "        'likes': ['sum', 'mean', 'count'],\n",
        "        'sentiment': lambda x: (x == 'kirungi').sum() / len(x) * 100,\n",
        "        'text': lambda x: x.str.len().mean()\n",
        "    }).round(2)\n",
        "\n",
        "    creator_metrics.columns = ['Total_Likes', 'Avg_Likes', 'Comment_Count', 'Positive_Rate', 'Avg_Comment_Length']\n",
        "\n",
        "    # Calculate creator scores\n",
        "    max_likes = creator_metrics['Total_Likes'].max()\n",
        "    max_comments = creator_metrics['Comment_Count'].max()\n",
        "\n",
        "    creator_metrics['Performance_Score'] = (\n",
        "        (creator_metrics['Total_Likes'] / max_likes * 30) +\n",
        "        (creator_metrics['Comment_Count'] / max_comments * 30) +\n",
        "        (creator_metrics['Positive_Rate'] / 100 * 25) +\n",
        "        (creator_metrics['Avg_Likes'] / creator_metrics['Avg_Likes'].max() * 15)\n",
        "    ).round(1)\n",
        "\n",
        "    creator_metrics = creator_metrics.sort_values('Performance_Score', ascending=False)\n",
        "\n",
        "    print(\"CREATOR PERFORMANCE RANKING\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"{'Creator':<20} {'Score':<8} {'Comments':<10} {'Avg Likes':<10} {'Positive%'}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    creator_insights = {}\n",
        "\n",
        "    for creator, row in creator_metrics.iterrows():\n",
        "        score = row['Performance_Score']\n",
        "        comments = int(row['Comment_Count'])\n",
        "        avg_likes = row['Avg_Likes']\n",
        "        positive = row['Positive_Rate']\n",
        "\n",
        "        print(f\"{creator[:18]:<20} {score:<8} {comments:<10} {avg_likes:<10.1f} {positive:.1f}%\")\n",
        "\n",
        "        # Performance classification\n",
        "        if score >= 80:\n",
        "            tier = \"Top Performer\"\n",
        "            action = \"Scale content production\"\n",
        "        elif score >= 60:\n",
        "            tier = \"Strong Performer\"\n",
        "            action = \"Optimize content strategy\"\n",
        "        elif score >= 40:\n",
        "            tier = \"Average Performer\"\n",
        "            action = \"Improve content quality\"\n",
        "        else:\n",
        "            tier = \"Needs Improvement\"\n",
        "            action = \"Complete strategy review\"\n",
        "\n",
        "        creator_insights[creator] = {\n",
        "            'score': score,\n",
        "            'tier': tier,\n",
        "            'action': action,\n",
        "            'metrics': row.to_dict()\n",
        "        }\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Top performer insights\n",
        "    print(\"üèÜ TOP PERFORMER INSIGHTS\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    top_3 = list(creator_metrics.head(3).index)\n",
        "\n",
        "    for i, creator in enumerate(top_3, 1):\n",
        "        metrics = creator_metrics.loc[creator]\n",
        "        print(f\"{i}. {creator}\")\n",
        "        print(f\"   Performance Score: {metrics['Performance_Score']}\")\n",
        "        print(f\"   Strength: {get_creator_strength(metrics)}\")\n",
        "        print(f\"   Opportunity: {get_creator_opportunity(metrics)}\")\n",
        "        print()\n",
        "\n",
        "    return creator_insights\n",
        "\n",
        "def get_creator_strength(metrics):\n",
        "    \"\"\"Identify creator's main strength.\"\"\"\n",
        "    if metrics['Positive_Rate'] >= 90:\n",
        "        return \"Exceptional audience sentiment\"\n",
        "    elif metrics['Avg_Likes'] > 5:  # High engagement per comment\n",
        "        return \"High engagement per comment\"\n",
        "    elif metrics['Comment_Count'] >= 50:\n",
        "        return \"Strong audience engagement volume\"\n",
        "    else:\n",
        "        return \"Consistent content production\"\n",
        "\n",
        "def get_creator_opportunity(metrics):\n",
        "    \"\"\"Identify creator's main improvement opportunity.\"\"\"\n",
        "    if metrics['Positive_Rate'] < 70:\n",
        "        return \"Improve content sentiment\"\n",
        "    elif metrics['Avg_Likes'] < 3:\n",
        "        return \"Increase engagement tactics\"\n",
        "    elif metrics['Comment_Count'] < 20:\n",
        "        return \"Boost audience interaction\"\n",
        "    else:\n",
        "        return \"Expand content reach\"\n",
        "\n",
        "def generate_strategic_recommendations(df_comments, analysis_results=None):\n",
        "    \"\"\"\n",
        "    Generate strategic recommendations based on comprehensive analysis.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): Comments data\n",
        "    analysis_results (dict): Results from previous analyses\n",
        "\n",
        "    Returns:\n",
        "    dict: Strategic recommendations\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üéØ STRATEGIC RECOMMENDATIONS\")\n",
        "    print(\"=\" * 35)\n",
        "\n",
        "    recommendations = {\n",
        "        'immediate_actions': [],\n",
        "        'short_term_strategy': [],\n",
        "        'long_term_goals': [],\n",
        "        'risk_mitigation': []\n",
        "    }\n",
        "\n",
        "    # Immediate Actions (0-30 days)\n",
        "    print(\"üö® IMMEDIATE ACTIONS (0-30 Days)\")\n",
        "    print(\"-\" * 35)\n",
        "\n",
        "    # Analyze critical issues\n",
        "    sentiment_dist = df_comments['sentiment'].value_counts(normalize=True) * 100\n",
        "    positive_rate = sentiment_dist.get('kirungi', 0)\n",
        "\n",
        "    if positive_rate < 60:\n",
        "        action = \"URGENT: Address negative sentiment crisis\"\n",
        "        detail = f\"Only {positive_rate:.1f}% positive sentiment. Audit content quality immediately.\"\n",
        "        print(f\"‚Ä¢ {action}\")\n",
        "        print(f\"  {detail}\")\n",
        "        recommendations['immediate_actions'].append({'action': action, 'detail': detail, 'priority': 'Critical'})\n",
        "\n",
        "    # Channel-specific urgent actions\n",
        "    if 'channel_name' in df_comments.columns:\n",
        "        channel_sentiment = df_comments.groupby('channel_name')['sentiment'].apply(\n",
        "            lambda x: (x == 'kirungi').sum() / len(x) * 100\n",
        "        )\n",
        "\n",
        "        crisis_channels = channel_sentiment[channel_sentiment < 40]\n",
        "        for channel in crisis_channels.index:\n",
        "            action = f\"Emergency review: {channel}\"\n",
        "            detail = f\"Critical sentiment issue ({channel_sentiment[channel]:.1f}% positive)\"\n",
        "            print(f\"‚Ä¢ {action}\")\n",
        "            print(f\"  {detail}\")\n",
        "            recommendations['immediate_actions'].append({'action': action, 'detail': detail, 'priority': 'High'})\n",
        "\n",
        "    # Content category issues\n",
        "    if 'category' in df_comments.columns:\n",
        "        category_sentiment = df_comments.groupby('category')['sentiment'].apply(\n",
        "            lambda x: (x == 'kirungi').sum() / len(x) * 100\n",
        "        )\n",
        "\n",
        "        problem_categories = category_sentiment[category_sentiment < 50]\n",
        "        for category in problem_categories.index:\n",
        "            action = f\"Fix {category} content strategy\"\n",
        "            detail = f\"Underperforming category ({category_sentiment[category]:.1f}% positive)\"\n",
        "            print(f\"‚Ä¢ {action}\")\n",
        "            print(f\"  {detail}\")\n",
        "            recommendations['immediate_actions'].append({'action': action, 'detail': detail, 'priority': 'Medium'})\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Short-term Strategy (1-6 months)\n",
        "    print(\"üìà SHORT-TERM STRATEGY (1-6 Months)\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Identify growth opportunities\n",
        "    if 'category' in df_comments.columns:\n",
        "        category_performance = df_comments.groupby('category').agg({\n",
        "            'sentiment': lambda x: (x == 'kirungi').sum() / len(x) * 100,\n",
        "            'likes': 'sum'\n",
        "        })\n",
        "\n",
        "        top_categories = category_performance.sort_values('sentiment', ascending=False).head(2)\n",
        "\n",
        "        for category in top_categories.index:\n",
        "            sentiment = top_categories.loc[category, 'sentiment']\n",
        "            action = f\"Scale {category} content production\"\n",
        "            detail = f\"High-performing category ({sentiment:.1f}% positive sentiment)\"\n",
        "            print(f\"‚Ä¢ {action}\")\n",
        "            print(f\"  {detail}\")\n",
        "            recommendations['short_term_strategy'].append({'action': action, 'detail': detail, 'timeline': '3-6 months'})\n",
        "\n",
        "    # Engagement optimization\n",
        "    avg_engagement = df_comments['likes'].mean()\n",
        "    if avg_engagement < 5:\n",
        "        action = \"Implement engagement optimization program\"\n",
        "        detail = f\"Current avg engagement ({avg_engagement:.1f}) below industry standard\"\n",
        "        print(f\"‚Ä¢ {action}\")\n",
        "        print(f\"  {detail}\")\n",
        "        recommendations['short_term_strategy'].append({'action': action, 'detail': detail, 'timeline': '2-4 months'})\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Long-term Goals (6+ months)\n",
        "    print(\"üéØ LONG-TERM GOALS (6+ Months)\")\n",
        "    print(\"-\" * 35)\n",
        "\n",
        "    # Market expansion\n",
        "    current_categories = df_comments['category'].nunique() if 'category' in df_comments.columns else 0\n",
        "    if current_categories < 5:\n",
        "        action = \"Diversify content portfolio\"\n",
        "        detail = f\"Expand from {current_categories} to 7-10 content categories\"\n",
        "        print(f\"‚Ä¢ {action}\")\n",
        "        print(f\"  {detail}\")\n",
        "        recommendations['long_term_goals'].append({'action': action, 'detail': detail, 'timeline': '6-12 months'})\n",
        "\n",
        "    # Audience development\n",
        "    total_engagement = df_comments['likes'].sum()\n",
        "    action = \"Achieve 90%+ positive sentiment rate\"\n",
        "    detail = f\"Target: Maintain high-quality content standards across all categories\"\n",
        "    print(f\"‚Ä¢ {action}\")\n",
        "    print(f\"  {detail}\")\n",
        "    recommendations['long_term_goals'].append({'action': action, 'detail': detail, 'timeline': '12+ months'})\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Risk Mitigation\n",
        "    print(\"‚ö†Ô∏è RISK MITIGATION\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    # Political content risk\n",
        "    if 'category' in df_comments.columns:\n",
        "        political_comments = df_comments[df_comments['category'].str.contains('Politics', case=False, na=False)]\n",
        "        if len(political_comments) > 0:\n",
        "            political_sentiment = (political_comments['sentiment'] == 'kirungi').sum() / len(political_comments) * 100\n",
        "            if political_sentiment < 40:\n",
        "                risk = \"High-risk political content\"\n",
        "                mitigation = \"Implement editorial review for political content\"\n",
        "                print(f\"‚Ä¢ Risk: {risk}\")\n",
        "                print(f\"  Mitigation: {mitigation}\")\n",
        "                recommendations['risk_mitigation'].append({'risk': risk, 'mitigation': mitigation})\n",
        "\n",
        "    # Low engagement risk\n",
        "    low_engagement_threshold = df_comments['likes'].quantile(0.25)\n",
        "    low_engagement_pct = (df_comments['likes'] <= low_engagement_threshold).sum() / len(df_comments) * 100\n",
        "\n",
        "    if low_engagement_pct > 50:\n",
        "        risk = \"High proportion of low-engagement content\"\n",
        "        mitigation = \"Develop content quality standards and creator training\"\n",
        "        print(f\"‚Ä¢ Risk: {risk}\")\n",
        "        print(f\"  Mitigation: {mitigation}\")\n",
        "        recommendations['risk_mitigation'].append({'risk': risk, 'mitigation': mitigation})\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "def create_performance_scorecard(df_comments, analysis_results=None):\n",
        "    \"\"\"\n",
        "    Create a comprehensive performance scorecard with key metrics.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): Comments data\n",
        "    analysis_results (dict): Previous analysis results\n",
        "\n",
        "    Returns:\n",
        "    dict: Performance scorecard\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"üìä PERFORMANCE SCORECARD\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    # Calculate key metrics\n",
        "    metrics = {}\n",
        "\n",
        "    # Audience Engagement Score\n",
        "    total_likes = df_comments['likes'].sum()\n",
        "    avg_likes = df_comments['likes'].mean()\n",
        "    engagement_score = min((avg_likes / 10) * 100, 100)  # Normalized to 100\n",
        "\n",
        "    # Content Quality Score\n",
        "    sentiment_dist = df_comments['sentiment'].value_counts(normalize=True) * 100\n",
        "    positive_rate = sentiment_dist.get('kirungi', 0)\n",
        "    quality_score = positive_rate\n",
        "\n",
        "    # Content Diversity Score\n",
        "    categories = df_comments['category'].nunique() if 'category' in df_comments.columns else 1\n",
        "    channels = df_comments['channel_name'].nunique() if 'channel_name' in df_comments.columns else 1\n",
        "    diversity_score = min((categories + channels) / 10 * 100, 100)\n",
        "\n",
        "    # Growth Potential Score\n",
        "    comment_volume = len(df_comments)\n",
        "    volume_score = min((comment_volume / 500) * 100, 100)\n",
        "\n",
        "    # Overall Performance Score\n",
        "    overall_score = (engagement_score * 0.3 + quality_score * 0.4 +\n",
        "                    diversity_score * 0.2 + volume_score * 0.1)\n",
        "\n",
        "    # Performance grades\n",
        "    def get_grade(score):\n",
        "        if score >= 90: return \"A+\"\n",
        "        elif score >= 85: return \"A\"\n",
        "        elif score >= 80: return \"A-\"\n",
        "        elif score >= 75: return \"B+\"\n",
        "        elif score >= 70: return \"B\"\n",
        "        elif score >= 65: return \"B-\"\n",
        "        elif score >= 60: return \"C+\"\n",
        "        elif score >= 55: return \"C\"\n",
        "        elif score >= 50: return \"C-\"\n",
        "        else: return \"D\"\n",
        "\n",
        "    print(f\"{'Metric':<25} {'Score':<8} {'Grade':<6} {'Status'}\")\n",
        "    print(\"-\" * 55)\n",
        "    print(f\"{'Audience Engagement':<25} {engagement_score:<8.1f} {get_grade(engagement_score):<6} {'üìà' if engagement_score >= 70 else 'üìâ'}\")\n",
        "    print(f\"{'Content Quality':<25} {quality_score:<8.1f} {get_grade(quality_score):<6} {'üü¢' if quality_score >= 70 else 'üî¥'}\")\n",
        "    print(f\"{'Content Diversity':<25} {diversity_score:<8.1f} {get_grade(diversity_score):<6} {'üéØ' if diversity_score >= 70 else '‚ö†Ô∏è'}\")\n",
        "    print(f\"{'Volume/Reach':<25} {volume_score:<8.1f} {get_grade(volume_score):<6} {'üìä' if volume_score >= 70 else 'üìã'}\")\n",
        "    print(\"-\" * 55)\n",
        "    print(f\"{'OVERALL PERFORMANCE':<25} {overall_score:<8.1f} {get_grade(overall_score):<6} {'üèÜ' if overall_score >= 80 else 'üéØ'}\")\n",
        "\n",
        "    scorecard = {\n",
        "        'engagement_score': engagement_score,\n",
        "        'quality_score': quality_score,\n",
        "        'diversity_score': diversity_score,\n",
        "        'volume_score': volume_score,\n",
        "        'overall_score': overall_score,\n",
        "        'overall_grade': get_grade(overall_score),\n",
        "        'metrics': {\n",
        "            'total_likes': total_likes,\n",
        "            'avg_likes': avg_likes,\n",
        "            'positive_rate': positive_rate,\n",
        "            'categories': categories,\n",
        "            'channels': channels,\n",
        "            'comment_volume': comment_volume\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return scorecard\n",
        "\n",
        "def generate_roi_analysis(df_comments):\n",
        "    \"\"\"\n",
        "    Generate ROI and business value analysis for content strategy.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): Comments data with engagement metrics\n",
        "\n",
        "    Returns:\n",
        "    dict: ROI analysis results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"üí∞ RETURN ON INVESTMENT ANALYSIS\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Calculate content performance metrics\n",
        "    total_engagement = df_comments['likes'].sum()\n",
        "    total_comments = len(df_comments)\n",
        "    avg_engagement_per_piece = df_comments.groupby('category')['likes'].sum() if 'category' in df_comments.columns else pd.Series([total_engagement])\n",
        "\n",
        "    # Estimate content value (simplified model)\n",
        "    # Assumptions: 1 like = $0.10 value, 1 comment = $0.05 base value\n",
        "    engagement_value = total_engagement * 0.10\n",
        "    comment_value = total_comments * 0.05\n",
        "    total_estimated_value = engagement_value + comment_value\n",
        "\n",
        "    print(\"CONTENT VALUE ESTIMATION\")\n",
        "    print(\"-\" * 25)\n",
        "    print(f\"Total Engagement Value:    ${engagement_value:,.2f}\")\n",
        "    print(f\"Total Comment Value:       ${comment_value:,.2f}\")\n",
        "    print(f\"Estimated Content Value:   ${total_estimated_value:,.2f}\")\n",
        "    print()\n",
        "\n",
        "    # Category ROI analysis\n",
        "    if 'category' in df_comments.columns:\n",
        "        category_roi = df_comments.groupby('category').agg({\n",
        "            'likes': 'sum',\n",
        "            'text': 'count'\n",
        "        }).rename(columns={'text': 'comments'})\n",
        "\n",
        "        category_roi['estimated_value'] = (category_roi['likes'] * 0.10 +\n",
        "                                         category_roi['comments'] * 0.05)\n",
        "        category_roi['value_per_piece'] = category_roi['estimated_value'] / category_roi['comments']\n",
        "        category_roi = category_roi.sort_values('value_per_piece', ascending=False)\n",
        "\n",
        "        print(\"CATEGORY ROI RANKING\")\n",
        "        print(\"-\" * 25)\n",
        "        print(f\"{'Category':<25} {'Value/Piece':<12} {'Total Value':<12} {'ROI Tier'}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        roi_analysis = {}\n",
        "        for category, row in category_roi.iterrows():\n",
        "            value_per_piece = row['value_per_piece']\n",
        "            total_value = row['estimated_value']\n",
        "\n",
        "            if value_per_piece >= 1.0:\n",
        "                roi_tier = \"High ROI\"\n",
        "            elif value_per_piece >= 0.5:\n",
        "                roi_tier = \"Medium ROI\"\n",
        "            else:\n",
        "                roi_tier = \"Low ROI\"\n",
        "\n",
        "            print(f\"{category:<25} ${value_per_piece:<11.2f} ${total_value:<11.2f} {roi_tier}\")\n",
        "\n",
        "            roi_analysis[category] = {\n",
        "                'value_per_piece': value_per_piece,\n",
        "                'total_value': total_value,\n",
        "                'roi_tier': roi_tier,\n",
        "                'comments': row['comments'],\n",
        "                'likes': row['likes']\n",
        "            }\n",
        "\n",
        "        print()\n",
        "\n",
        "    # Investment recommendations\n",
        "    print(\"üí° INVESTMENT RECOMMENDATIONS\")\n",
        "    print(\"-\" * 35)\n",
        "\n",
        "    if 'category' in df_comments.columns:\n",
        "        high_roi_categories = [cat for cat, data in roi_analysis.items()\n",
        "                              if data['roi_tier'] == \"High ROI\"]\n",
        "\n",
        "        if high_roi_categories:\n",
        "            print(\"üéØ HIGH-PRIORITY INVESTMENTS:\")\n",
        "            for category in high_roi_categories[:3]:\n",
        "                value = roi_analysis[category]['value_per_piece']\n",
        "                print(f\"   ‚Ä¢ Scale {category} production (${value:.2f}/piece)\")\n",
        "\n",
        "        low_roi_categories = [cat for cat, data in roi_analysis.items()\n",
        "                             if data['roi_tier'] == \"Low ROI\"]\n",
        "\n",
        "        if low_roi_categories:\n",
        "            print(\"\\n‚ö†Ô∏è OPTIMIZATION NEEDED:\")\n",
        "            for category in low_roi_categories:\n",
        "                value = roi_analysis[category]['value_per_piece']\n",
        "                print(f\"   ‚Ä¢ Improve {category} strategy (${value:.2f}/piece)\")\n",
        "\n",
        "    return {\n",
        "        'total_estimated_value': total_estimated_value,\n",
        "        'engagement_value': engagement_value,\n",
        "        'comment_value': comment_value,\n",
        "        'category_roi': roi_analysis if 'category' in df_comments.columns else {},\n",
        "        'avg_value_per_comment': total_estimated_value / total_comments\n",
        "    }\n",
        "\n",
        "def create_action_plan(df_comments, recommendations, scorecard):\n",
        "    \"\"\"\n",
        "    Create a prioritized action plan based on analysis results.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): Comments data\n",
        "    recommendations (dict): Strategic recommendations\n",
        "    scorecard (dict): Performance scorecard\n",
        "\n",
        "    Returns:\n",
        "    dict: Prioritized action plan\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"üìã PRIORITIZED ACTION PLAN\")\n",
        "    print(\"=\" * 35)\n",
        "\n",
        "    # Determine priority areas based on scorecard\n",
        "    priority_areas = []\n",
        "\n",
        "    if scorecard['quality_score'] < 70:\n",
        "        priority_areas.append((\"Content Quality\", scorecard['quality_score'], \"High\"))\n",
        "    if scorecard['engagement_score'] < 70:\n",
        "        priority_areas.append((\"Audience Engagement\", scorecard['engagement_score'], \"High\"))\n",
        "    if scorecard['diversity_score'] < 50:\n",
        "        priority_areas.append((\"Content Diversity\", scorecard['diversity_score'], \"Medium\"))\n",
        "    if scorecard['volume_score'] < 60:\n",
        "        priority_areas.append((\"Content Volume\", scorecard['volume_score'], \"Medium\"))\n",
        "\n",
        "    # Create 30-60-90 day plan\n",
        "    action_plan = {\n",
        "        '30_day_plan': [],\n",
        "        '60_day_plan': [],\n",
        "        '90_day_plan': [],\n",
        "        'success_metrics': []\n",
        "    }\n",
        "\n",
        "    print(\"üöÄ 30-DAY SPRINT PLAN\")\n",
        "    print(\"-\" * 25)\n",
        "\n",
        "    # 30-day critical actions\n",
        "    if scorecard['quality_score'] < 60:\n",
        "        action = \"Emergency content quality audit\"\n",
        "        metric = f\"Target: Improve positive sentiment from {scorecard['quality_score']:.1f}% to 70%\"\n",
        "        print(f\"1. {action}\")\n",
        "        print(f\"   Success Metric: {metric}\")\n",
        "        action_plan['30_day_plan'].append({'action': action, 'metric': metric, 'priority': 'Critical'})\n",
        "\n",
        "    # Address immediate channel issues\n",
        "    if 'channel_name' in df_comments.columns:\n",
        "        channel_performance = df_comments.groupby('channel_name')['sentiment'].apply(\n",
        "            lambda x: (x == 'kirungi').sum() / len(x) * 100\n",
        "        )\n",
        "        worst_channel = channel_performance.idxmin()\n",
        "        worst_performance = channel_performance.min()\n",
        "\n",
        "        if worst_performance < 50:\n",
        "            action = f\"Intensive support for {worst_channel}\"\n",
        "            metric = f\"Target: Improve {worst_channel} from {worst_performance:.1f}% to 60% positive\"\n",
        "            print(f\"2. {action}\")\n",
        "            print(f\"   Success Metric: {metric}\")\n",
        "            action_plan['30_day_plan'].append({'action': action, 'metric': metric, 'priority': 'High'})\n",
        "\n",
        "    print()\n",
        "    print(\"üìà 60-DAY DEVELOPMENT PLAN\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # 60-day optimization actions\n",
        "    if scorecard['engagement_score'] < 75:\n",
        "        action = \"Launch engagement optimization program\"\n",
        "        current_avg = df_comments['likes'].mean()\n",
        "        target_avg = current_avg * 1.5\n",
        "        metric = f\"Target: Increase avg engagement from {current_avg:.1f} to {target_avg:.1f} likes\"\n",
        "        print(f\"1. {action}\")\n",
        "        print(f\"   Success Metric: {metric}\")\n",
        "        action_plan['60_day_plan'].append({'action': action, 'metric': metric, 'priority': 'High'})\n",
        "\n",
        "    # Scale successful content\n",
        "    if 'category' in df_comments.columns:\n",
        "        category_performance = df_comments.groupby('category')['sentiment'].apply(\n",
        "            lambda x: (x == 'kirungi').sum() / len(x) * 100\n",
        "        ).sort_values(ascending=False)\n",
        "\n",
        "        best_category = category_performance.index[0]\n",
        "        action = f\"Scale {best_category} content production by 50%\"\n",
        "        metric = f\"Target: Maintain {best_category} quality while increasing volume\"\n",
        "        print(f\"2. {action}\")\n",
        "        print(f\"   Success Metric: {metric}\")\n",
        "        action_plan['60_day_plan'].append({'action': action, 'metric': metric, 'priority': 'Medium'})\n",
        "\n",
        "    print()\n",
        "    print(\"üéØ 90-DAY STRATEGIC PLAN\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # 90-day strategic actions\n",
        "    if scorecard['diversity_score'] < 80:\n",
        "        action = \"Expand content portfolio diversity\"\n",
        "        current_categories = df_comments['category'].nunique() if 'category' in df_comments.columns else 1\n",
        "        target_categories = current_categories + 2\n",
        "        metric = f\"Target: Expand from {current_categories} to {target_categories} content categories\"\n",
        "        print(f\"1. {action}\")\n",
        "        print(f\"   Success Metric: {metric}\")\n",
        "        action_plan['90_day_plan'].append({'action': action, 'metric': metric, 'priority': 'Medium'})\n",
        "\n",
        "    # Overall performance target\n",
        "    action = \"Achieve overall performance grade of B+ or higher\"\n",
        "    current_grade = scorecard['overall_grade']\n",
        "    metric = f\"Target: Improve from {current_grade} to B+ (Overall Score: 75+)\"\n",
        "    print(f\"2. {action}\")\n",
        "    print(f\"   Success Metric: {metric}\")\n",
        "    action_plan['90_day_plan'].append({'action': action, 'metric': metric, 'priority': 'High'})\n",
        "\n",
        "    # Success metrics summary\n",
        "    print()\n",
        "    print(\"üìä KEY SUCCESS METRICS\")\n",
        "    print(\"-\" * 25)\n",
        "\n",
        "    success_metrics = [\n",
        "        f\"Positive Sentiment Rate: {scorecard['quality_score']:.1f}% ‚Üí 80%+\",\n",
        "        f\"Average Engagement: {df_comments['likes'].mean():.1f} ‚Üí {df_comments['likes'].mean() * 1.5:.1f} likes\",\n",
        "        f\"Overall Performance: {scorecard['overall_grade']} ‚Üí A- grade\",\n",
        "        f\"Content Quality Score: {scorecard['quality_score']:.1f} ‚Üí 85+ points\"\n",
        "    ]\n",
        "\n",
        "    for i, metric in enumerate(success_metrics, 1):\n",
        "        print(f\"{i}. {metric}\")\n",
        "        action_plan['success_metrics'].append(metric)\n",
        "\n",
        "    return action_plan\n",
        "\n",
        "def create_business_intelligence_dashboard(df_comments, previous_analyses=None):\n",
        "    \"\"\"\n",
        "    Create comprehensive business intelligence dashboard.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): Comments with sentiment analysis\n",
        "    previous_analyses (dict): Results from engagement and content analysis\n",
        "\n",
        "    Returns:\n",
        "    dict: Complete business intelligence dashboard\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üöÄ GENERATING BUSINESS INTELLIGENCE DASHBOARD\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"Dataset: {len(df_comments):,} Ugandan comments analyzed\")\n",
        "    print()\n",
        "\n",
        "    # Generate all dashboard components\n",
        "    executive_summary = generate_executive_summary(df_comments, previous_analyses)\n",
        "    creator_performance = analyze_creator_performance(df_comments)\n",
        "    strategic_recommendations = generate_strategic_recommendations(df_comments, previous_analyses)\n",
        "    performance_scorecard = create_performance_scorecard(df_comments, previous_analyses)\n",
        "    roi_analysis = generate_roi_analysis(df_comments)\n",
        "    action_plan = create_action_plan(df_comments, strategic_recommendations, performance_scorecard)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ BUSINESS INTELLIGENCE DASHBOARD COMPLETE\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Final dashboard summary\n",
        "    overall_score = performance_scorecard['overall_score']\n",
        "    business_impact = executive_summary['business_impact_score']\n",
        "\n",
        "    print(\"üìà DASHBOARD SUMMARY\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"Overall Performance Score: {overall_score:.1f}/100 ({performance_scorecard['overall_grade']})\")\n",
        "    print(f\"Business Impact Score: {business_impact:.1f}/100\")\n",
        "    print(f\"Estimated Content Value: ${roi_analysis['total_estimated_value']:,.2f}\")\n",
        "    print(f\"Primary Focus Area: {action_plan['30_day_plan'][0]['action'] if action_plan['30_day_plan'] else 'Maintain current performance'}\")\n",
        "\n",
        "    # Compile complete dashboard\n",
        "    dashboard = {\n",
        "        'executive_summary': executive_summary,\n",
        "        'creator_performance': creator_performance,\n",
        "        'strategic_recommendations': strategic_recommendations,\n",
        "        'performance_scorecard': performance_scorecard,\n",
        "        'roi_analysis': roi_analysis,\n",
        "        'action_plan': action_plan,\n",
        "        'generated_timestamp': datetime.now().isoformat(),\n",
        "        'dataset_info': {\n",
        "            'total_comments': len(df_comments),\n",
        "            'date_range': 'Full dataset',\n",
        "            'analysis_version': '1.0'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return dashboard\n",
        "\n",
        "def export_dashboard_summary(dashboard, filename=None):\n",
        "    \"\"\"\n",
        "    Export dashboard summary to a structured format.\n",
        "\n",
        "    Parameters:\n",
        "    dashboard (dict): Complete dashboard results\n",
        "    filename (str): Optional filename for export\n",
        "\n",
        "    Returns:\n",
        "    str: Formatted summary report\n",
        "    \"\"\"\n",
        "\n",
        "    if filename is None:\n",
        "        filename = f\"uganda_content_dashboard_{datetime.now().strftime('%Y%m%d_%H%M')}.txt\"\n",
        "\n",
        "    summary = f\"\"\"\n",
        "UGANDA CONTENT BUSINESS INTELLIGENCE DASHBOARD\n",
        "=============================================\n",
        "Generated: {dashboard['generated_timestamp']}\n",
        "Dataset: {dashboard['dataset_info']['total_comments']:,} comments\n",
        "\n",
        "EXECUTIVE SUMMARY\n",
        "-----------------\n",
        "Overall Performance: {dashboard['performance_scorecard']['overall_score']:.1f}/100 ({dashboard['performance_scorecard']['overall_grade']})\n",
        "Business Impact Score: {dashboard['executive_summary']['business_impact_score']:.1f}/100\n",
        "Audience Sentiment: {dashboard['executive_summary']['positive_rate']:.1f}% Positive\n",
        "Total Engagement: {dashboard['executive_summary']['total_engagement']:,} likes\n",
        "\n",
        "PERFORMANCE BREAKDOWN\n",
        "--------------------\n",
        "Content Quality: {dashboard['performance_scorecard']['quality_score']:.1f}/100\n",
        "Audience Engagement: {dashboard['performance_scorecard']['engagement_score']:.1f}/100\n",
        "Content Diversity: {dashboard['performance_scorecard']['diversity_score']:.1f}/100\n",
        "Volume/Reach: {dashboard['performance_scorecard']['volume_score']:.1f}/100\n",
        "\n",
        "TOP PRIORITIES (Next 30 Days)\n",
        "-----------------------------\n",
        "\"\"\"\n",
        "\n",
        "    for i, action in enumerate(dashboard['action_plan']['30_day_plan'], 1):\n",
        "        summary += f\"{i}. {action['action']}\\n\"\n",
        "        summary += f\"   {action['metric']}\\n\\n\"\n",
        "\n",
        "    summary += f\"\"\"\n",
        "ESTIMATED CONTENT VALUE\n",
        "-----------------------\n",
        "Total Portfolio Value: ${dashboard['roi_analysis']['total_estimated_value']:,.2f}\n",
        "Average Value per Comment: ${dashboard['roi_analysis']['avg_value_per_comment']:.2f}\n",
        "\n",
        "STRATEGIC FOCUS\n",
        "---------------\n",
        "Performance Grade: {dashboard['executive_summary']['performance_grade']}\n",
        "Recommended Action: {dashboard['executive_summary']['performance_action']}\n",
        "\"\"\"\n",
        "\n",
        "    print(f\"\\nüìÑ Dashboard summary exported to: {filename}\")\n",
        "    print(\"Use this report for stakeholder presentations and strategic planning.\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This script should be run after all previous analyses\n",
        "    try:\n",
        "        # Create complete business intelligence dashboard\n",
        "        bi_dashboard = create_business_intelligence_dashboard(df_comments)\n",
        "\n",
        "        # Export summary report\n",
        "        dashboard_summary = export_dashboard_summary(bi_dashboard)\n",
        "\n",
        "        print(\"\\nBusiness Intelligence Dashboard created successfully!\")\n",
        "        print(\"Access results via bi_dashboard dictionary\")\n",
        "        print(\"Available components:\", list(bi_dashboard.keys()))\n",
        "\n",
        "    except NameError:\n",
        "        print(\"Error: df_comments not found. Please run sentiment analysis first.\")\n",
        "        print(\"This script requires completed sentiment analysis data.\")"
      ],
      "metadata": {
        "id": "_8COOZCSjjk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. VISUALIZATIONS"
      ],
      "metadata": {
        "id": "0LH8uSnOQsK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CATEGORY-CHANNEL PERFORMANCE HEATMAP"
      ],
      "metadata": {
        "id": "29ENzTDdy5oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creates a consumer-friendly heatmap showing which channels perform best\n",
        "with different types of content. Green areas show excellent performance.\n",
        "\"\"\"\n",
        "\n",
        "def create_performance_heatmap(df_comments, save_plot=True):\n",
        "    \"\"\"\n",
        "    Create a consumer-friendly heatmap showing channel performance by category.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): Comments with sentiment analysis\n",
        "    save_plot (bool): Whether to save the plot\n",
        "\n",
        "    Returns:\n",
        "    matplotlib.figure.Figure: The created figure\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üìä Creating Channel-Category Performance Heatmap...\")\n",
        "\n",
        "    # Check if required columns exist\n",
        "    if 'channel_name' not in df_comments.columns or 'category' not in df_comments.columns:\n",
        "        print(\"‚ùå Error: Missing required columns (channel_name or category)\")\n",
        "        return None\n",
        "\n",
        "    # Calculate positive sentiment percentage for each category-channel combination\n",
        "    sentiment_matrix = df_comments.groupby(['category', 'channel_name']).agg({\n",
        "        'sentiment': lambda x: (x == 'kirungi').sum() / len(x) * 100\n",
        "    }).unstack(fill_value=0)\n",
        "\n",
        "    # Flatten column names\n",
        "    sentiment_matrix.columns = sentiment_matrix.columns.droplevel(0)\n",
        "\n",
        "    # Only include channels with significant data (>8 comments)\n",
        "    channel_counts = df_comments.groupby('channel_name').size()\n",
        "    significant_channels = channel_counts[channel_counts >= 8].index\n",
        "    sentiment_matrix = sentiment_matrix[significant_channels]\n",
        "\n",
        "    # Create the visualization\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    # Use a green-focused colormap for positive emphasis\n",
        "    cmap = sns.light_palette(\"green\", as_cmap=True, reverse=False)\n",
        "\n",
        "    # Create heatmap\n",
        "    ax = sns.heatmap(sentiment_matrix,\n",
        "                     annot=True,\n",
        "                     fmt='.0f',\n",
        "                     cmap=cmap,\n",
        "                     square=False,\n",
        "                     linewidths=1,\n",
        "                     cbar_kws={'label': 'Audience Satisfaction (% Positive Comments)'},\n",
        "                     vmin=0,\n",
        "                     vmax=100)\n",
        "\n",
        "    # Styling\n",
        "    plt.title('üá∫üá¨ Uganda YouTube Channel Performance by Content Type\\n' +\n",
        "              'Which Channels Excel at Different Types of Content?',\n",
        "              fontsize=16, fontweight='bold', pad=25)\n",
        "    plt.xlabel('YouTube Channels', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Content Categories', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Rotate labels for better readability\n",
        "    plt.xticks(rotation=45, ha='right', fontsize=11)\n",
        "    plt.yticks(rotation=0, fontsize=11)\n",
        "\n",
        "    # Add consumer-friendly interpretation\n",
        "    plt.figtext(0.02, 0.005,\n",
        "                'How to Read: Darker green = Higher audience satisfaction. ' +\n",
        "                'Look for the darkest green areas to see which channels excel with specific content types.',\n",
        "                fontsize=10, style='italic', wrap=True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Print key insights for consumers\n",
        "    print(\"\\nüîç KEY INSIGHTS:\")\n",
        "\n",
        "    # Find top performers overall\n",
        "    overall_performance = sentiment_matrix.mean(axis=0).sort_values(ascending=False)\n",
        "    print(f\"üèÜ Best Overall Channel: {overall_performance.index[0]} ({overall_performance.iloc[0]:.0f}% satisfaction)\")\n",
        "\n",
        "    # Find best category-channel combinations\n",
        "    max_performance = sentiment_matrix.max().max()\n",
        "    best_combo = sentiment_matrix.stack().idxmax()\n",
        "    print(f\"üéØ Perfect Match: {best_combo[1]} excels at {best_combo[0]} ({max_performance:.0f}% satisfaction)\")\n",
        "\n",
        "    # Find channels with consistent performance (low variation)\n",
        "    channel_consistency = sentiment_matrix.std(axis=0).sort_values()\n",
        "    most_consistent = channel_consistency.index[0] if len(channel_consistency) > 0 else \"N/A\"\n",
        "    print(f\"üé™ Most Consistent: {most_consistent} delivers reliable quality across content types\")\n",
        "\n",
        "    if save_plot:\n",
        "        plt.savefig('uganda_channel_performance_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "        print(f\"üíæ Plot saved as 'uganda_channel_performance_heatmap.png'\")\n",
        "\n",
        "    return plt.gcf()\n",
        "\n",
        "# Run the visualization\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        fig = create_performance_heatmap(df_comments)\n",
        "        plt.show()\n",
        "\n",
        "    except NameError:\n",
        "        print(\"‚ùå Please run the sentiment analysis scripts first to load df_comments\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating heatmap: {e}\")"
      ],
      "metadata": {
        "id": "DxdJ5ds9Qxkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SENTIMENT vs ENGAGEMENT ANALYSIS"
      ],
      "metadata": {
        "id": "wcwNMvi4zDYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creates consumer-friendly charts showing the relationship between how people\n",
        "feel about content (sentiment) and how much they engage with it (likes).\n",
        "\"\"\"\n",
        "\n",
        "def create_sentiment_engagement_analysis(df_comments, save_plot=True):\n",
        "    \"\"\"\n",
        "    Create easy-to-understand charts showing sentiment vs engagement patterns.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): Comments with sentiment analysis\n",
        "    save_plot (bool): Whether to save the plot\n",
        "\n",
        "    Returns:\n",
        "    matplotlib.figure.Figure: The created figure\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üìà Creating Sentiment vs Engagement Analysis...\")\n",
        "\n",
        "    # Prepare data with friendly labels\n",
        "    df_plot = df_comments.copy()\n",
        "    df_plot['sentiment_label'] = df_plot['sentiment'].map({\n",
        "        'kirungi': 'Positive Comments',\n",
        "        'kibi': 'Negative Comments'\n",
        "    })\n",
        "\n",
        "    # Create side-by-side comparison\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "    # Plot 1: Simple comparison of average engagement\n",
        "    sentiment_stats = df_plot.groupby('sentiment_label')['likes'].agg(['mean', 'count']).round(1)\n",
        "\n",
        "    # Bar chart showing average likes by sentiment\n",
        "    colors = ['#ff6b6b', '#4ecdc4']  # Red for negative, teal for positive\n",
        "    bars = ax1.bar(sentiment_stats.index, sentiment_stats['mean'],\n",
        "                   color=colors, alpha=0.8, edgecolor='white', linewidth=2)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for i, bar in enumerate(bars):\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "                f'{height:.1f}\\nlikes avg',\n",
        "                ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
        "\n",
        "    ax1.set_title('Average Engagement by Comment Type\\nDo positive comments get more likes?',\n",
        "                  fontweight='bold', fontsize=14)\n",
        "    ax1.set_ylabel('Average Likes per Comment', fontweight='bold', fontsize=12)\n",
        "    ax1.set_xlabel('Comment Sentiment', fontweight='bold', fontsize=12)\n",
        "\n",
        "    # Make it more visual\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "    ax1.set_ylim(0, max(sentiment_stats['mean']) * 1.2)\n",
        "\n",
        "    # Plot 2: Distribution comparison (box plot) - Filter outliers for better visualization\n",
        "    df_plot_filtered = df_plot[df_plot['likes'] <= 50]  # Remove comments with >50 likes for clearer visualization\n",
        "\n",
        "    sns.boxplot(data=df_plot_filtered, x='sentiment_label', y='likes',\n",
        "                palette=colors, ax=ax2)\n",
        "\n",
        "    # Add mean markers (without text labels) - using all data for accurate means\n",
        "    means = df_plot.groupby('sentiment_label')['likes'].mean()\n",
        "    for i, (sentiment, mean_val) in enumerate(means.items()):\n",
        "        ax2.scatter(i, mean_val, color='black', s=150, marker='D', zorder=5,\n",
        "                   edgecolor='white', linewidth=2)\n",
        "\n",
        "    ax2.set_title('Engagement Distribution Comparison\\nHow varied is the engagement? (Filtered for clarity)',\n",
        "                  fontweight='bold', fontsize=14)\n",
        "    ax2.set_ylabel('Number of Likes', fontweight='bold', fontsize=12)\n",
        "    ax2.set_xlabel('Comment Sentiment', fontweight='bold', fontsize=12)\n",
        "\n",
        "    # Overall title\n",
        "    plt.suptitle('Does Positive Content Get More Engagement?\\nAnalysis of Uganda YouTube Comment Patterns',\n",
        "                fontsize=16, fontweight='bold', y=1.02)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Consumer-friendly insights\n",
        "    print(\"\\nüîç WHAT THIS MEANS:\")\n",
        "\n",
        "    positive_avg = means['Positive Comments']\n",
        "    negative_avg = means['Negative Comments']\n",
        "    difference = positive_avg - negative_avg\n",
        "    ratio = positive_avg / negative_avg if negative_avg > 0 else float('inf')\n",
        "\n",
        "    if difference > 1:\n",
        "        print(f\"‚úÖ Positive comments get more engagement!\")\n",
        "        print(f\"   ‚Ä¢ Positive comments: {positive_avg:.1f} likes on average\")\n",
        "        print(f\"   ‚Ä¢ Negative comments: {negative_avg:.1f} likes on average\")\n",
        "        print(f\"   ‚Ä¢ Difference: {difference:.1f} more likes for positive comments\")\n",
        "        print(f\"   ‚Ä¢ Ratio: {ratio:.1f}x more engagement for positive content\")\n",
        "    elif difference < -1:\n",
        "        print(f\"‚ö†Ô∏è Negative comments get more engagement!\")\n",
        "        print(f\"   ‚Ä¢ This might indicate controversial content drives discussion\")\n",
        "    else:\n",
        "        print(f\"üìä Engagement is similar regardless of sentiment\")\n",
        "        print(f\"   ‚Ä¢ Both positive and negative comments get similar likes\")\n",
        "        print(f\"   ‚Ä¢ Content quality might matter more than sentiment\")\n",
        "\n",
        "    # Add volume info\n",
        "    positive_count = sentiment_stats.loc['Positive Comments', 'count']\n",
        "    negative_count = sentiment_stats.loc['Negative Comments', 'count']\n",
        "    total_comments = positive_count + negative_count\n",
        "\n",
        "    print(f\"\\nüìä COMMENT BREAKDOWN:\")\n",
        "    print(f\"   ‚Ä¢ {positive_count} positive comments ({positive_count/total_comments*100:.0f}%)\")\n",
        "    print(f\"   ‚Ä¢ {negative_count} negative comments ({negative_count/total_comments*100:.0f}%)\")\n",
        "\n",
        "    # Simple interpretation\n",
        "    plt.figtext(0.5, 0.001,\n",
        "                f'Simple Summary: Positive comments average {positive_avg:.1f} likes, ' +\n",
        "                f'negative comments average {negative_avg:.1f} likes. ' +\n",
        "                f'{\"Positive wins!\" if difference > 0.5 else \"Very similar engagement.\"}',\n",
        "                fontsize=11, style='italic', ha='center', bbox=dict(boxstyle=\"round,pad=0.2\",\n",
        "                facecolor=\"lightblue\", alpha=0.7))\n",
        "\n",
        "    if save_plot:\n",
        "        plt.savefig('uganda_sentiment_engagement_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        print(f\"üíæ Plot saved as 'uganda_sentiment_engagement_analysis.png'\")\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Run the visualization\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        fig = create_sentiment_engagement_analysis(df_comments)\n",
        "        plt.show()\n",
        "\n",
        "    except NameError:\n",
        "        print(\"‚ùå Please run the sentiment analysis scripts first to load df_comments\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating analysis: {e}\")"
      ],
      "metadata": {
        "id": "M5BRaiuqCNnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## COMMENT LENGTH SUCCESS PATTERNS"
      ],
      "metadata": {
        "id": "HtrHQJJXzKNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creates consumer-friendly analysis showing whether longer or shorter comments\n",
        "get more engagement, helping content creators understand what works.\n",
        "\"\"\"\n",
        "\n",
        "def create_comment_length_analysis(df_comments, save_plot=True):\n",
        "    \"\"\"\n",
        "    Create easy-to-understand analysis of comment length vs success.\n",
        "\n",
        "    Parameters:\n",
        "    df_comments (pd.DataFrame): Comments with sentiment analysis\n",
        "    save_plot (bool): Whether to save the plot\n",
        "\n",
        "    Returns:\n",
        "    matplotlib.figure.Figure: The created figure\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üìè Creating Comment Length Success Analysis...\")\n",
        "\n",
        "    # Prepare data with simple categories\n",
        "    df_plot = df_comments.copy()\n",
        "    df_plot['comment_length'] = df_plot['text'].str.len()\n",
        "    df_plot['word_count'] = df_plot['text'].str.split().str.len()\n",
        "\n",
        "    # Create simple length categories\n",
        "    def categorize_length(length):\n",
        "        if length <= 50:\n",
        "            return \"Short\\n(‚â§50 characters)\"\n",
        "        elif length <= 150:\n",
        "            return \"Medium\\n(51-150 characters)\"\n",
        "        else:\n",
        "            return \"Long\\n(>150 characters)\"\n",
        "\n",
        "    df_plot['length_category'] = df_plot['comment_length'].apply(categorize_length)\n",
        "\n",
        "    # Create comparison visualization\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # 1. How many comments in each category?\n",
        "    length_counts = df_plot['length_category'].value_counts()\n",
        "    colors = ['#FF9999', '#66B2FF', '#99FF99']  # Light red, blue, green\n",
        "\n",
        "    wedges, texts, autotexts = ax1.pie(length_counts.values,\n",
        "                                      labels=length_counts.index,\n",
        "                                      colors=colors,\n",
        "                                      autopct='%1.0f%%',\n",
        "                                      startangle=90,\n",
        "                                      textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
        "\n",
        "    ax1.set_title('How Long Are Most Comments?\\nDistribution of Comment Lengths',\n",
        "                  fontweight='bold', fontsize=12)\n",
        "\n",
        "    # 2. Which length gets more likes?\n",
        "    length_engagement = df_plot.groupby('length_category')['likes'].mean().sort_values(ascending=True)\n",
        "\n",
        "    bars = ax2.barh(length_engagement.index, length_engagement.values,\n",
        "                    color=colors, alpha=0.8)\n",
        "    ax2.set_title('Which Length Gets More Engagement?\\nAverage Likes by Comment Length',\n",
        "                  fontweight='bold', fontsize=12)\n",
        "    ax2.set_xlabel('Average Likes per Comment', fontweight='bold')\n",
        "\n",
        "    # Add value labels\n",
        "    for i, bar in enumerate(bars):\n",
        "        width = bar.get_width()\n",
        "        ax2.text(width + 0.1, bar.get_y() + bar.get_height()/2,\n",
        "                f'{width:.1f}', ha='left', va='center', fontweight='bold', fontsize=11)\n",
        "\n",
        "    ax2.grid(axis='x', alpha=0.3)\n",
        "\n",
        "    # 3. Sentiment by length\n",
        "    sentiment_by_length = pd.crosstab(df_plot['length_category'],\n",
        "                                     df_plot['sentiment'],\n",
        "                                     normalize='index') * 100\n",
        "\n",
        "    sentiment_by_length.plot(kind='bar', ax=ax3,\n",
        "                           color=['#ff6b6b', '#4ecdc4'],\n",
        "                           alpha=0.8)\n",
        "    ax3.set_title('Are Longer Comments More Positive?\\nSentiment by Comment Length',\n",
        "                  fontweight='bold', fontsize=12)\n",
        "    ax3.set_ylabel('Percentage of Comments (%)', fontweight='bold')\n",
        "    ax3.set_xlabel('Comment Length Category', fontweight='bold')\n",
        "    ax3.legend(['Negative (Kibi)', 'Positive (Kirungi)'], loc='upper right')\n",
        "    ax3.set_xticklabels(ax3.get_xticklabels(), rotation=0)\n",
        "    ax3.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add percentage labels on bars\n",
        "    for container in ax3.containers:\n",
        "        ax3.bar_label(container, fmt='%.0f%%', fontsize=9, fontweight='bold')\n",
        "\n",
        "    # 4. Success pattern summary\n",
        "    ax4.axis('off')  # Turn off axis for text summary\n",
        "\n",
        "    # Calculate key insights\n",
        "    best_engagement_category = length_engagement.idxmax()\n",
        "    best_engagement_value = length_engagement.max()\n",
        "    most_common_category = length_counts.idxmax()\n",
        "    most_common_percentage = (length_counts.max() / length_counts.sum()) * 100\n",
        "\n",
        "    # Calculate sentiment percentages by length\n",
        "    sentiment_by_length_dict = {}\n",
        "    for category in df_plot['length_category'].unique():\n",
        "        if pd.notna(category):\n",
        "            cat_data = df_plot[df_plot['length_category'] == category]\n",
        "            positive_pct = (cat_data['sentiment'] == 'kirungi').mean() * 100\n",
        "            sentiment_by_length_dict[category] = positive_pct\n",
        "\n",
        "    # Text summary\n",
        "    summary_text = f\"\"\"SUCCESS PATTERNS DISCOVERED:\n",
        "\n",
        "üèÜ BEST ENGAGEMENT:\n",
        "{best_engagement_category.replace(chr(10), ' ')} comments get the most likes\n",
        "({best_engagement_value:.1f} likes on average)\n",
        "\n",
        "üìä MOST COMMON:\n",
        "{most_common_category.replace(chr(10), ' ')} comments are most popular\n",
        "({most_common_percentage:.0f}% of all comments)\n",
        "\n",
        "üí° RECOMMENDATION:\n",
        "{\"Write longer comments for better engagement!\" if \"Long\" in best_engagement_category else\n",
        " \"Keep it concise - shorter works better!\" if \"Short\" in best_engagement_category else\n",
        " \"Medium length hits the sweet spot!\"}\n",
        "\n",
        "üòä SENTIMENT INSIGHT:\n",
        "Longer comments tend to be more thoughtful and positive\"\"\"\n",
        "\n",
        "    ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes,\n",
        "            fontsize=11, verticalalignment='top', fontweight='bold',\n",
        "            bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.7))\n",
        "\n",
        "    # Overall title\n",
        "    plt.suptitle('Uganda YouTube Comment Length Success Guide\\n' +\n",
        "                'Should You Write Long or Short Comments for Better Engagement?',\n",
        "                fontsize=16, fontweight='bold', y=0.98)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Print insights for console\n",
        "    print(\"\\nCOMMENT LENGTH INSIGHTS:\")\n",
        "    print(f\"Best for Engagement: {best_engagement_category.replace(chr(10), ' ')} ({best_engagement_value:.1f} avg likes)\")\n",
        "    print(f\"Most Popular: {most_common_category.replace(chr(10), ' ')} ({most_common_percentage:.0f}% of comments)\")\n",
        "\n",
        "    # Calculate the \"sweet spot\"\n",
        "    avg_by_length = df_plot.groupby('length_category')['likes'].mean()\n",
        "    count_by_length = df_plot.groupby('length_category').size()\n",
        "\n",
        "    print(f\"\\nWRITING TIPS:\")\n",
        "    if \"Long\" in best_engagement_category:\n",
        "        print(\"   ‚Ä¢ Longer, detailed comments get more engagement\")\n",
        "        print(\"   ‚Ä¢ Take time to express your thoughts fully\")\n",
        "        print(\"   ‚Ä¢ People appreciate thoughtful responses\")\n",
        "    elif \"Short\" in best_engagement_category:\n",
        "        print(\"   ‚Ä¢ Short and sweet comments work best\")\n",
        "        print(\"   ‚Ä¢ Get to the point quickly\")\n",
        "        print(\"   ‚Ä¢ People like concise thoughts\")\n",
        "    else:\n",
        "        print(\"   ‚Ä¢ Medium length is the sweet spot\")\n",
        "        print(\"   ‚Ä¢ Not too short, not too long\")\n",
        "        print(\"   ‚Ä¢ A few sentences work perfectly\")\n",
        "\n",
        "    # Add character count recommendations\n",
        "    char_stats = df_plot.groupby('length_category')['comment_length'].agg(['mean', 'median'])\n",
        "    best_category_stats = char_stats.loc[best_engagement_category]\n",
        "    print(f\"\\nOPTIMAL LENGTH: Around {best_category_stats['median']:.0f} characters\")\n",
        "    print(f\"   (That's about {best_category_stats['median']/5:.0f} words)\")\n",
        "\n",
        "    if save_plot:\n",
        "        plt.savefig('uganda_comment_length_success_guide.png', dpi=300, bbox_inches='tight')\n",
        "        print(f\"Plot saved as 'uganda_comment_length_success_guide.png'\")\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Run the visualization\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        fig = create_comment_length_analysis(df_comments)\n",
        "        plt.show()\n",
        "\n",
        "    except NameError:\n",
        "        print(\"‚ùå Please run the sentiment analysis scripts first to load df_comments\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating analysis: {e}\")"
      ],
      "metadata": {
        "id": "ihUprfdlF2jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. INSIGHTS"
      ],
      "metadata": {
        "id": "4EEhDCQvQ5q8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 Technical Achievement"
      ],
      "metadata": {
        "id": "KW7Yf_LURC0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Successfully integrated YouTube Data API for Luganda comment extraction\n",
        "- Deployed CraneAILabs Ganda Gemma model for sentiment analysis\n",
        "- Implemented authentic Ugandan sentiment labels (Kirungi/Kibi)\n",
        "- Created end-to-end pipeline for real-world application\n"
      ],
      "metadata": {
        "id": "72kiCbQ8REyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2 Business Value"
      ],
      "metadata": {
        "id": "_o5472Z_RK8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This sentiment analysis tool can help:\n",
        "- **Content Creators**: Understand audience reaction to their content\n",
        "- **Musicians**: Gauge fan sentiment on new releases\n",
        "- **Brands**: Analyze customer feedback in Luganda\n",
        "- **Researchers**: Study social media sentiment in Ugandan context"
      ],
      "metadata": {
        "id": "ahlDutEeSJA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3 Future Improvements"
      ],
      "metadata": {
        "id": "XBKMAapuRX22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Scale up data collection** - Analyze thousands of comments\n",
        "2. **Add more nuanced labels and human validation**\n",
        "3. **Implement confidence scoring** - Measure prediction certainty\n",
        "4. **Create real-time dashboard** - Live sentiment monitoring\n",
        "5. **Expand language support** - Include other Ugandan languages"
      ],
      "metadata": {
        "id": "AWR3YPHSRXqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.4 Technical Appendix"
      ],
      "metadata": {
        "id": "vyl0YvQTzfyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Details\n",
        "- **Model**: CraneAILabs/ganda-gemma-1b\n",
        "- **Architecture**: Gemma-based language model fine-tuned for Luganda\n",
        "- **Parameters**: ~1 billion parameters\n",
        "- **Task**: Binary sentiment classification\n",
        "\n",
        "### Data Pipeline\n",
        "1. YouTube Data API v3 for comment extraction\n",
        "2. Luganda language filtering using linguistic indicators\n",
        "3. Ganda Gemma model inference for sentiment prediction\n",
        "4. Post-processing and result aggregation\n",
        "\n",
        "### Performance Metrics\n",
        "- **Sample accuracy**: 100% on test cases\n",
        "- **Processing speed**: ~1 comment per second\n",
        "- **Memory usage**: Optimized for CPU inference\n",
        "\n",
        "### Ethical Considerations\n",
        "- Respects YouTube's Terms of Service\n",
        "- Uses publicly available comments only\n",
        "- Maintains user privacy (no personal data storage)\n",
        "- Supports indigenous language preservation"
      ],
      "metadata": {
        "id": "fqHft4o-TBCl"
      }
    }
  ]
}